<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture 8 è¶…åŸºç¡€å¤ä¹ ç¬”è®° - è¯åµŒå…¥ï¼ˆWord Embeddingsï¼‰</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", "Helvetica Neue", Helvetica, Arial, sans-serif;
            line-height: 1.8;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #e8f5e9 0%, #f1f8e9 100%);
            color: #1b5e20;
        }
        h1 {
            color: #2e7d32;
            border-bottom: 4px solid #4caf50;
            padding-bottom: 10px;
            font-size: 2.2em;
            text-align: center;
            background: linear-gradient(135deg, #4caf50, #66bb6a);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        h2 {
            color: #388e3c;
            border-left: 5px solid #66bb6a;
            padding-left: 15px;
            margin-top: 30px;
            background-color: #f1f8f6;
            padding: 10px 15px;
            border-radius: 5px;
        }
        h3 {
            color: #43a047;
            margin-top: 20px;
            border-bottom: 2px solid #81c784;
            padding-bottom: 5px;
        }
        h4 {
            color: #558b2f;
            margin-top: 15px;
        }
        pre {
            background-color: #f1f8f6;
            border: 2px solid #a5d6a7;
            border-left: 5px solid #4caf50;
            border-radius: 5px;
            padding: 15px;
            overflow-x: auto;
            font-family: "Monaco", "Menlo", "Consolas", monospace;
            font-size: 0.9em;
            line-height: 1.6;
            box-shadow: 0 2px 5px rgba(76, 175, 80, 0.1);
        }
        code {
            background-color: #e8f5e9;
            color: #2e7d32;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: "Monaco", "Menlo", "Consolas", monospace;
            font-size: 0.9em;
        }
        pre code {
            background-color: transparent;
            padding: 0;
        }
        strong {
            color: #1b5e20;
            font-weight: 600;
        }
        ul, ol {
            margin-left: 20px;
        }
        li {
            margin: 8px 0;
        }
        hr {
            border: none;
            border-top: 2px solid #a5d6a7;
            margin: 30px 0;
        }
        blockquote {
            border-left: 4px solid #66bb6a;
            padding-left: 15px;
            margin-left: 0;
            color: #33691e;
            background-color: #f1f8f6;
            padding: 10px 15px;
            border-radius: 5px;
        }
        details {
            background-color: #f1f8f6;
            border: 2px solid #a5d6a7;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
        }
        summary {
            font-weight: bold;
            color: #2e7d32;
            cursor: pointer;
            padding: 5px;
        }
        summary:hover {
            background-color: #e8f5e9;
            border-radius: 3px;
        }
        .highlight {
            background-color: #c8e6c9;
            padding: 2px 6px;
            border-radius: 3px;
            font-weight: 500;
        }
        a {
            color: #2e7d32;
            text-decoration: none;
            border-bottom: 1px solid #4caf50;
        }
        a:hover {
            color: #1b5e20;
            border-bottom: 2px solid #2e7d32;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #a5d6a7;
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #4caf50;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f1f8f6;
        }
    </style>
</head>
<body>
    <h1>Lecture 8 è¶…åŸºç¡€å¤ä¹ ç¬”è®° - è¯åµŒå…¥ï¼ˆWord Embeddingsï¼‰</h1>

    <h2>é›¶ã€å…ˆç†è§£é—®é¢˜ï¼ˆä»€ä¹ˆæ˜¯è¯åµŒå…¥ï¼Ÿï¼‰</h2>

    <h3>ä¼ ç»Ÿæ–¹æ³•çš„é—®é¢˜</h3>

    <p><strong>One-hotç¼–ç ï¼š</strong></p>

    <pre><code>è¯æ±‡è¡¨ï¼š['cat', 'dog', 'king', 'queen']

cat   = [1, 0, 0, 0]
dog   = [0, 1, 0, 0]
king  = [0, 0, 1, 0]
queen = [0, 0, 0, 1]</code></pre>

    <p><strong>é—®é¢˜ï¼š</strong></p>

    <pre><code>1. ç»´åº¦å¤ªé«˜
   10ä¸‡ä¸ªè¯ â†’ 10ä¸‡ç»´å‘é‡

2. æ— æ³•è¡¨ç¤ºè¯­ä¹‰å…³ç³»
   catå’Œdogéƒ½æ˜¯åŠ¨ç‰©ï¼Œä½†å‘é‡å®Œå…¨ä¸ç›¸å…³
   kingå’Œqueenéƒ½æ˜¯royaltyï¼Œä½†å‘é‡ä¹Ÿå®Œå…¨ä¸ç›¸å…³

3. ç¨€ç–æ€§
   å‘é‡ä¸­åªæœ‰ä¸€ä¸ª1ï¼Œå…¶ä½™éƒ½æ˜¯0</code></pre>

    <hr>

    <h3>è¯åµŒå…¥çš„è§£å†³æ–¹æ¡ˆ</h3>

    <p><strong>å®šä¹‰ï¼š</strong>å°†è¯æ˜ å°„åˆ°ä½ç»´ç¨ å¯†å‘é‡</p>

    <p><strong>ä¾‹å­ï¼š</strong></p>

    <pre><code># 3ç»´è¯åµŒå…¥ï¼ˆå®é™…é€šå¸¸100-300ç»´ï¼‰
cat   = [0.2, 0.8, 0.1]  â† åŠ¨ç‰©ç‰¹å¾å¼º
dog   = [0.3, 0.7, 0.2]  â† ä¹Ÿæ˜¯åŠ¨ç‰©ï¼Œå‘é‡ç›¸ä¼¼
king  = [0.9, 0.1, 0.8]  â† royaltyç‰¹å¾
queen = [0.8, 0.2, 0.9]  â† ä¹Ÿæ˜¯royaltyï¼Œå‘é‡ç›¸ä¼¼</code></pre>

    <p><strong>ä¼˜ç‚¹ï¼š</strong></p>

    <pre><code>1. ä½ç»´åº¦
   10ä¸‡ä¸ªè¯ â†’ 300ç»´å‘é‡ï¼ˆå‡å°‘333å€ï¼‰

2. è¯­ä¹‰ç›¸ä¼¼
   ç›¸ä¼¼çš„è¯æœ‰ç›¸ä¼¼çš„å‘é‡
   cat â‰ˆ dogï¼ˆéƒ½æ˜¯åŠ¨ç‰©ï¼‰
   king â‰ˆ queenï¼ˆéƒ½æ˜¯royaltyï¼‰

3. ç¨ å¯†
   å‘é‡ä¸­å¤§éƒ¨åˆ†å…ƒç´ éƒ½æœ‰æ„ä¹‰</code></pre>

    <hr>

    <h2>ä¸€ã€Word2VecåŸç†</h2>

    <h3>1. æ ¸å¿ƒæ€æƒ³</h3>

    <p><strong>åˆ†å¸ƒå‡è¯´ï¼ˆDistributional Hypothesisï¼‰ï¼š</strong></p>

    <pre><code>"You shall know a word by the company it keeps"
ï¼ˆé€šè¿‡ä¸€ä¸ªè¯çš„é‚»å±…æ¥äº†è§£å®ƒï¼‰

ä¾‹å­ï¼š
"The cat sat on the mat"
"The dog sat on the rug"

catå’Œdogå‡ºç°åœ¨ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­
â†’ å®ƒä»¬åº”è¯¥æœ‰ç›¸ä¼¼çš„æ„ä¹‰
â†’ å®ƒä»¬åº”è¯¥æœ‰ç›¸ä¼¼çš„å‘é‡</code></pre>

    <hr>

    <h3>2. Word2Vecçš„ä¸¤ç§æ¨¡å‹</h3>

    <h4>CBOWï¼ˆContinuous Bag of Wordsï¼‰</h4>

    <p><strong>ç›®æ ‡ï¼š</strong>ç”¨ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯</p>

    <pre><code>è¾“å…¥ï¼šä¸Šä¸‹æ–‡è¯
è¾“å‡ºï¼šä¸­å¿ƒè¯

ä¾‹å­ï¼š
ä¸Šä¸‹æ–‡ï¼š["The", "cat", "on", "the", "mat"]
é¢„æµ‹ï¼šsat

ä¸Šä¸‹æ–‡ï¼š["The", "dog", "on", "the", "rug"]
é¢„æµ‹ï¼šsat</code></pre>

    <hr>

    <h4>Skip-gram</h4>

    <p><strong>ç›®æ ‡ï¼š</strong>ç”¨ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡</p>

    <pre><code>è¾“å…¥ï¼šä¸­å¿ƒè¯
è¾“å‡ºï¼šä¸Šä¸‹æ–‡è¯

ä¾‹å­ï¼š
è¾“å…¥ï¼šsat
é¢„æµ‹ï¼š["The", "cat", "on", "the", "mat"]

è¾“å…¥ï¼šsat
é¢„æµ‹ï¼š["The", "dog", "on", "the", "rug"]</code></pre>

    <hr>

    <h3>3. è®­ç»ƒè¿‡ç¨‹ï¼ˆç®€åŒ–ï¼‰</h3>

    <pre><code>æ­¥éª¤1ï¼šéšæœºåˆå§‹åŒ–è¯å‘é‡
cat   = [random, random, random]
dog   = [random, random, random]
...

æ­¥éª¤2ï¼šéå†è®­ç»ƒæ–‡æœ¬
å¯¹äºå¥å­ï¼š"The cat sat on the mat"
  å¯¹äºæ¯ä¸ªè¯åŠå…¶ä¸Šä¸‹æ–‡
    è°ƒæ•´å‘é‡ï¼Œä½¿å¾—ï¼š
    - ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„è¯ï¼Œå‘é‡æ›´æ¥è¿‘
    - ä¸Šä¸‹æ–‡ä¸­ä¸å‡ºç°çš„è¯ï¼Œå‘é‡æ›´è¿œç¦»

æ­¥éª¤3ï¼šè¿­ä»£å¤šæ¬¡ï¼Œç›´åˆ°æ”¶æ•›

ç»“æœï¼š
catå’Œdogçš„å‘é‡å˜å¾—ç›¸ä¼¼ï¼ˆå› ä¸ºä¸Šä¸‹æ–‡ç›¸ä¼¼ï¼‰</code></pre>

    <hr>

    <h2>äºŒã€å‘é‡è¿ç®—</h2>

    <h3>1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰</h3>

    <p><strong>å…¬å¼ï¼š</strong></p>

    <pre><code>cos(Î¸) = (A Â· B) / (|A| Ã— |B|)

å…¶ä¸­ï¼š
A Â· B = Aå’ŒBçš„ç‚¹ç§¯
|A| = Açš„æ¨¡é•¿
|B| = Bçš„æ¨¡é•¿</code></pre>

    <p><strong>æ‰‹å·¥è®¡ç®—ç¤ºä¾‹ï¼š</strong></p>

    <pre><code>A = [1, 2, 3]
B = [4, 5, 6]

A Â· B = 1Ã—4 + 2Ã—5 + 3Ã—6 = 4 + 10 + 18 = 32

|A| = âˆš(1Â² + 2Â² + 3Â²) = âˆš14 â‰ˆ 3.74
|B| = âˆš(4Â² + 5Â² + 6Â²) = âˆš77 â‰ˆ 8.77

cos(Î¸) = 32 / (3.74 Ã— 8.77) = 32 / 32.8 â‰ˆ 0.976</code></pre>

    <p><strong>è§£é‡Šï¼š</strong></p>

    <pre><code>cos(Î¸) â‰ˆ 1  â†’ å‘é‡éå¸¸ç›¸ä¼¼ï¼ˆå¤¹è§’æ¥è¿‘0Â°ï¼‰
cos(Î¸) â‰ˆ 0  â†’ å‘é‡ä¸ç›¸å…³ï¼ˆå¤¹è§’æ¥è¿‘90Â°ï¼‰
cos(Î¸) â‰ˆ -1 â†’ å‘é‡ç›¸åï¼ˆå¤¹è§’æ¥è¿‘180Â°ï¼‰</code></pre>

    <p><strong>ä»£ç å®ç°ï¼š</strong></p>

    <pre><code>import numpy as np

def cosine_similarity(v1, v2):
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    # ç‚¹ç§¯
    dot_product = np.dot(v1, v2)

    # æ¨¡é•¿
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)

    # ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = dot_product / (norm_v1 * norm_v2)

    return similarity

# ä¾‹å­
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])
print(cosine_similarity(v1, v2))  # 0.974</code></pre>

    <hr>

    <h3>2. è¯­ä¹‰å‘é‡è¿ç®—</h3>

    <p><strong>è‘—åä¾‹å­ï¼š</strong></p>

    <pre><code>king - man + woman â‰ˆ queen</code></pre>

    <p><strong>ä¸ºä»€ä¹ˆå¯è¡Œï¼Ÿ</strong></p>

    <pre><code>kingçš„å‘é‡åŒ…å«ï¼š
  - royaltyï¼ˆç‹å®¤ï¼‰
  - maleï¼ˆç”·æ€§ï¼‰
  - powerï¼ˆæƒåŠ›ï¼‰

mançš„å‘é‡åŒ…å«ï¼š
  - maleï¼ˆç”·æ€§ï¼‰
  - humanï¼ˆäººç±»ï¼‰

womançš„å‘é‡åŒ…å«ï¼š
  - femaleï¼ˆå¥³æ€§ï¼‰
  - humanï¼ˆäººç±»ï¼‰

king - manï¼šå»é™¤"ç”·æ€§"ç‰¹å¾ï¼Œä¿ç•™"ç‹å®¤+æƒåŠ›"
king - man + womanï¼šåŠ ä¸Š"å¥³æ€§"ç‰¹å¾
ç»“æœï¼šroyalty + female = queen âœ“</code></pre>

    <p><strong>å…¶ä»–ä¾‹å­ï¼š</strong></p>

    <pre><code>Paris - France + Italy â‰ˆ Rome
ï¼ˆé¦–éƒ½å…³ç³»ï¼‰

bigger - big + small â‰ˆ smaller
ï¼ˆæ¯”è¾ƒçº§å…³ç³»ï¼‰

walking - walk + swim â‰ˆ swimming
ï¼ˆåŠ¨åè¯å…³ç³»ï¼‰</code></pre>

    <hr>

    <h2>ä¸‰ã€ä½¿ç”¨Gensimè®­ç»ƒWord2Vec</h2>

    <h3>1. å‡†å¤‡æ•°æ®</h3>

    <pre><code>import nltk
from nltk.corpus import gutenberg

# ä¸‹è½½Moby Dick
nltk.download('gutenberg')

# è¯»å–æ–‡æœ¬
moby_dick = gutenberg.sents('melville-moby_dick.txt')

# æ•°æ®æ ¼å¼ï¼š
# [['Call', 'me', 'Ishmael', '.'],
#  ['Some', 'years', 'ago', ...],
#  ...]</code></pre>

    <hr>

    <h3>2. è®­ç»ƒæ¨¡å‹</h3>

    <pre><code>from gensim.models import Word2Vec

# è®­ç»ƒæ¨¡å‹ï¼ˆä¸€è¡Œä»£ç ï¼ï¼‰
model = Word2Vec(
    sentences=moby_dick,  # è®­ç»ƒæ•°æ®
    vector_size=100,      # å‘é‡ç»´åº¦
    window=5,             # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    min_count=5,          # æœ€å°è¯é¢‘ï¼ˆä½äºæ­¤å€¼çš„è¯è¢«å¿½ç•¥ï¼‰
    workers=4             # å¹¶è¡Œçº¿ç¨‹æ•°
)</code></pre>

    <p><strong>å‚æ•°è§£é‡Šï¼š</strong></p>

    <pre><code>vector_size=100ï¼š
  æ¯ä¸ªè¯ç”¨100ç»´å‘é‡è¡¨ç¤º

window=5ï¼š
  è€ƒè™‘å‰å5ä¸ªè¯ä½œä¸ºä¸Šä¸‹æ–‡
  ä¾‹å¦‚ï¼š"The cat sat on the mat"
  å¯¹äº"sat"ï¼Œä¸Šä¸‹æ–‡æ˜¯["The", "cat", "on", "the", "mat"]

min_count=5ï¼š
  åªè®­ç»ƒå‡ºç°5æ¬¡ä»¥ä¸Šçš„è¯
  è¿‡æ»¤æ‰ç”Ÿåƒ»è¯å’Œæ‹¼å†™é”™è¯¯

workers=4ï¼š
  ä½¿ç”¨4ä¸ªCPUæ ¸å¿ƒå¹¶è¡Œè®­ç»ƒ
  åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹</code></pre>

    <hr>

    <h3>3. ä½¿ç”¨æ¨¡å‹</h3>

    <h4>è·å–è¯å‘é‡</h4>

    <pre><code># è·å–å•è¯çš„å‘é‡
whale_vec = model.wv['whale']
print(whale_vec.shape)  # (100,)
print(whale_vec[:5])    # [0.123, -0.456, 0.789, ...]</code></pre>

    <hr>

    <h4>æŸ¥æ‰¾ç›¸ä¼¼è¯</h4>

    <pre><code># æ‰¾æœ€ç›¸ä¼¼çš„10ä¸ªè¯
similar_words = model.wv.most_similar('whale', topn=10)
print(similar_words)

# è¾“å‡ºç¤ºä¾‹ï¼š
# [('ship', 0.95),
#  ('boat', 0.93),
#  ('sea', 0.91),
#  ('ocean', 0.89),
#  ...]</code></pre>

    <hr>

    <h4>è¯è¯­ç±»æ¯”</h4>

    <pre><code># king - man + woman â‰ˆ queen
result = model.wv.most_similar(
    positive=['king', 'woman'],  # åŠ ä¸Šè¿™äº›è¯
    negative=['man'],            # å‡å»è¿™äº›è¯
    topn=1
)
print(result)  # [('queen', 0.85)]</code></pre>

    <hr>

    <h3>4. å®ç°most_similaræ–¹æ³•</h3>

    <p><strong>ä»»åŠ¡ï¼š</strong>è‡ªå·±å®ç°ç±»ä¼¼Gensimçš„most_similaråŠŸèƒ½</p>

    <pre><code>def most_similar(word, model, topn=10):
    """
    æ‰¾å‡ºä¸ç»™å®šè¯æœ€ç›¸ä¼¼çš„topnä¸ªè¯

    å‚æ•°ï¼š
    wordï¼šç›®æ ‡è¯
    modelï¼šè®­ç»ƒå¥½çš„Word2Vecæ¨¡å‹
    topnï¼šè¿”å›å‰nä¸ªç»“æœ

    è¿”å›ï¼š
    [(word1, similarity1), (word2, similarity2), ...]
    """
    # è·å–ç›®æ ‡è¯çš„å‘é‡
    target_vec = model.wv[word]

    # å­˜å‚¨æ‰€æœ‰è¯çš„ç›¸ä¼¼åº¦
    similarities = []

    # éå†è¯æ±‡è¡¨ä¸­çš„æ‰€æœ‰è¯
    for vocab_word in model.wv.index_to_key:
        # è·³è¿‡ç›®æ ‡è¯æœ¬èº«
        if vocab_word == word:
            continue

        # è·å–å½“å‰è¯çš„å‘é‡
        current_vec = model.wv[vocab_word]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sim = cosine_similarity(target_vec, current_vec)

        # å­˜å‚¨
        similarities.append((vocab_word, sim))

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)

    # è¿”å›å‰topnä¸ª
    return similarities[:topn]

# ä½¿ç”¨
result = most_similar('ship', model, topn=5)
print(result)
# [('boat', 0.9945),
#  ('head', 0.9881),
#  ('boiling', 0.9821),
#  ...]</code></pre>

    <hr>

    <h2>å››ã€GloVeæ¨¡å‹</h2>

    <h3>1. GloVe vs Word2Vec</h3>

    <p><strong>Word2Vecï¼š</strong></p>
    <pre><code>åŸºäºä¸Šä¸‹æ–‡çª—å£
å±€éƒ¨ä¿¡æ¯

ä¾‹å¦‚ï¼šåœ¨"The cat sat"ä¸­
åªçœ‹åˆ°catå’Œsatçš„å±€éƒ¨å…±ç°</code></pre>

    <p><strong>GloVeï¼š</strong></p>
    <pre><code>åŸºäºå…¨å±€å…±ç°çŸ©é˜µ
å…¨å±€ç»Ÿè®¡ä¿¡æ¯

ç»Ÿè®¡æ•´ä¸ªè¯­æ–™åº“ä¸­
catå’Œsatå…±ç°äº†å¤šå°‘æ¬¡</code></pre>

    <hr>

    <h3>2. åŠ è½½é¢„è®­ç»ƒGloVeæ¨¡å‹</h3>

    <pre><code>import gensim.downloader as api

# ä¸‹è½½é¢„è®­ç»ƒçš„GloVeæ¨¡å‹ï¼ˆTwitteræ•°æ®ï¼‰
# è¿™ä¼šä¸‹è½½å¤§çº¦1GBçš„æ–‡ä»¶
glove_model = api.load('glove-twitter-25')

# ä½¿ç”¨æ–¹æ³•å’ŒWord2Vecç›¸åŒ
similar_words = glove_model.most_similar('happy', topn=5)
print(similar_words)
# [('glad', 0.89),
#  ('pleased', 0.87),
#  ('excited', 0.85),
#  ...]</code></pre>

    <p><strong>é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜ç‚¹ï¼š</strong></p>

    <pre><code>1. æ— éœ€è‡ªå·±è®­ç»ƒ
   â†’ èŠ‚çœæ—¶é—´å’Œè®¡ç®—èµ„æº

2. è®­ç»ƒæ•°æ®é‡å¤§
   â†’ 20äº¿Twitteræ¨æ–‡
   â†’ è¦†ç›–æ›´å¤šè¯æ±‡

3. è´¨é‡é«˜
   â†’ ä¸“ä¸šå›¢é˜Ÿè®­ç»ƒ
   â†’ å‚æ•°ä¼˜åŒ–å¥½</code></pre>

    <hr>

    <h2>äº”ã€æ–‡æœ¬åˆ†ç±»åº”ç”¨</h2>

    <h3>é—®é¢˜ï¼šå¦‚ä½•ç”¨è¯å‘é‡åˆ†ç±»å¥å­ï¼Ÿ</h3>

    <p><strong>æŒ‘æˆ˜ï¼š</strong></p>

    <pre><code>æœ‰è¯å‘é‡ï¼š
cat   = [0.2, 0.8, 0.1]
dog   = [0.3, 0.7, 0.2]
sat   = [0.5, 0.5, 0.5]

ä½†å¥å­æ€ä¹ˆè¡¨ç¤ºï¼Ÿ
"The cat sat" = ?</code></pre>

    <hr>

    <h3>è§£å†³æ–¹æ¡ˆï¼šPoolingï¼ˆæ± åŒ–ï¼‰</h3>

    <h4>Mean Poolingï¼ˆå¹³å‡æ± åŒ–ï¼‰</h4>

    <p><strong>æ–¹æ³•ï¼š</strong>å¯¹æ‰€æœ‰è¯å‘é‡æ±‚å¹³å‡</p>

    <pre><code>def mean_pooling(sentence, model):
    """
    è®¡ç®—å¥å­çš„å¹³å‡å‘é‡

    å‚æ•°ï¼š
    sentenceï¼šå¥å­ï¼ˆè¯åˆ—è¡¨ï¼‰
    modelï¼šè¯å‘é‡æ¨¡å‹

    è¿”å›ï¼š
    å¥å­å‘é‡
    """
    # å­˜å‚¨æ‰€æœ‰è¯çš„å‘é‡
    vectors = []

    for word in sentence:
        # å¦‚æœè¯åœ¨æ¨¡å‹ä¸­ï¼Œè·å–å…¶å‘é‡
        if word in model:
            vectors.append(model[word])

    # å¦‚æœæ²¡æœ‰è¯åœ¨æ¨¡å‹ä¸­ï¼Œè¿”å›é›¶å‘é‡
    if len(vectors) == 0:
        return np.zeros(model.vector_size)

    # è®¡ç®—å¹³å‡å€¼
    sentence_vec = np.mean(vectors, axis=0)

    return sentence_vec

# ä¾‹å­
sentence = ['the', 'cat', 'sat']
sentence_vec = mean_pooling(sentence, glove_model)
print(sentence_vec.shape)  # (25,) å¦‚æœç”¨25ç»´GloVe</code></pre>

    <p><strong>æ‰‹å·¥è®¡ç®—ç¤ºä¾‹ï¼š</strong></p>

    <pre><code>å‡è®¾3ç»´å‘é‡ï¼š
the = [0.1, 0.2, 0.3]
cat = [0.4, 0.5, 0.6]
sat = [0.7, 0.8, 0.9]

mean = (the + cat + sat) / 3
     = ([0.1, 0.2, 0.3] + [0.4, 0.5, 0.6] + [0.7, 0.8, 0.9]) / 3
     = [1.2, 1.5, 1.8] / 3
     = [0.4, 0.5, 0.6]</code></pre>

    <hr>

    <h4>Max Poolingï¼ˆæœ€å¤§æ± åŒ–ï¼‰</h4>

    <pre><code>def max_pooling(sentence, model):
    """æœ€å¤§æ± åŒ–ï¼šå–æ¯ä¸ªç»´åº¦çš„æœ€å¤§å€¼"""
    vectors = []
    for word in sentence:
        if word in model:
            vectors.append(model[word])

    if len(vectors) == 0:
        return np.zeros(model.vector_size)

    sentence_vec = np.max(vectors, axis=0)
    return sentence_vec</code></pre>

    <p><strong>æ‰‹å·¥è®¡ç®—ç¤ºä¾‹ï¼š</strong></p>

    <pre><code>the = [0.1, 0.2, 0.3]
cat = [0.4, 0.5, 0.6]
sat = [0.7, 0.8, 0.9]

max = [max(0.1, 0.4, 0.7),
       max(0.2, 0.5, 0.8),
       max(0.3, 0.6, 0.9)]
    = [0.7, 0.8, 0.9]</code></pre>

    <hr>

    <h3>æƒ…æ„Ÿåˆ†ç±»å®Œæ•´æµç¨‹</h3>

    <pre><code>import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
df = pd.read_csv('Tweets_short.csv')

# æ­¥éª¤2ï¼šé¢„å¤„ç†
def preprocess_tweet(text):
    """ç®€å•çš„é¢„å¤„ç†"""
    # è½¬å°å†™
    text = text.lower()
    # åˆ†è¯
    tokens = text.split()
    return tokens

df['tokens'] = df['text'].apply(preprocess_tweet)

# æ­¥éª¤3ï¼šæå–å¥å­å‘é‡
df['vector'] = df['tokens'].apply(
    lambda tokens: mean_pooling(tokens, glove_model)
)

# æ­¥éª¤4ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®
X = np.vstack(df['vector'].values)  # ç‰¹å¾çŸ©é˜µ
y = df['sentiment'].values          # æ ‡ç­¾

# æ­¥éª¤5ï¼šåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æ­¥éª¤6ï¼šè®­ç»ƒåˆ†ç±»å™¨
classifier = SVC(kernel='linear')
classifier.fit(X_train, y_train)

# æ­¥éª¤7ï¼šé¢„æµ‹
y_pred = classifier.predict(X_test)

# æ­¥éª¤8ï¼šè¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2%}')

# æ‰“å°è¯¦ç»†æŠ¥å‘Š
print(classification_report(y_test, y_pred))</code></pre>

    <hr>

    <h3>é¢„å¤„ç†Twitterç‰¹å¾</h3>

    <p><strong>é—®é¢˜ï¼š</strong>Twitteræ–‡æœ¬æœ‰ç‰¹æ®Šå…ƒç´ </p>

    <pre><code>"@user I love #python! Check this out: http://..."</code></pre>

    <p><strong>GloVeæ¨¡å‹çš„é¢„å¤„ç†ï¼š</strong></p>

    <pre><code>@user â†’ &lt;USER&gt;
#python â†’ &lt;HASHTAG&gt;
http://... â†’ &lt;URL&gt;</code></pre>

    <p><strong>å®ç°ï¼š</strong></p>

    <pre><code>import re

def preprocess_tweet_glove(text):
    """
    æŒ‰ç…§GloVeæ¨¡å‹çš„é¢„å¤„ç†æ–¹å¼å¤„ç†Twitteræ–‡æœ¬
    """
    # æ›¿æ¢URL
    text = re.sub(r'https?://\S+', '&lt;URL&gt;', text)

    # æ›¿æ¢@mentions
    text = re.sub(r'@\w+', '&lt;USER&gt;', text)

    # æ›¿æ¢#hashtags
    text = re.sub(r'#(\w+)', '&lt;HASHTAG&gt;', text)

    # è½¬å°å†™
    text = text.lower()

    # åˆ†è¯
    tokens = text.split()

    return tokens

# ä¾‹å­
text = "@john I love #python! Check http://example.com"
print(preprocess_tweet_glove(text))
# ['&lt;USER&gt;', 'i', 'love', '&lt;HASHTAG&gt;', '!', 'check', '&lt;URL&gt;']</code></pre>

    <hr>

    <h2>å…­ã€é”™è¯¯åˆ†æ</h2>

    <h3>æ··æ·†çŸ©é˜µ</h3>

    <pre><code>from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred,
                      labels=['positive', 'neutral', 'negative'])

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d',
            xticklabels=['positive', 'neutral', 'negative'],
            yticklabels=['positive', 'neutral', 'negative'])
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()</code></pre>

    <hr>

    <h3>åˆ†æé”™è¯¯æ ·æœ¬</h3>

    <pre><code># æ‰¾å‡ºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬
df_test = df.loc[X_test.index]
df_test['predicted'] = y_pred
df_test['correct'] = (y_test == y_pred)

# é”™è¯¯æ ·æœ¬
errors = df_test[df_test['correct'] == False]

# åˆ†æï¼šnegativeè¢«é¢„æµ‹ä¸ºpositive
neg_as_pos = errors[(errors['sentiment'] == 'negative') &
                    (errors['predicted'] == 'positive')]

print(f"Negativeé¢„æµ‹ä¸ºPositiveçš„æ ·æœ¬æ•°ï¼š{len(neg_as_pos)}")
print("\nç¤ºä¾‹ï¼š")
for idx, row in neg_as_pos.head(5).iterrows():
    print(f"æ–‡æœ¬ï¼š{row['text']}")
    print(f"çœŸå®ï¼š{row['sentiment']}, é¢„æµ‹ï¼š{row['predicted']}\n")</code></pre>

    <hr>

    <h2>ä¸ƒã€çº¸ç¬”è€ƒè¯•é‡ç‚¹</h2>

    <h3>å¿…é¡»æŒæ¡</h3>

    <h4>1. âœ… <strong>è¯åµŒå…¥çš„å®šä¹‰</strong></h4>

    <pre><code>å®šä¹‰ï¼šå°†è¯æ˜ å°„åˆ°ä½ç»´ç¨ å¯†å‘é‡
ç›®çš„ï¼šæ•æ‰è¯çš„è¯­ä¹‰ä¿¡æ¯</code></pre>

    <h4>2. âœ… <strong>ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—</strong></h4>

    <pre><code>å…¬å¼ï¼šcos(Î¸) = (AÂ·B) / (|A|Ã—|B|)

ä¼šæ‰‹å·¥è®¡ç®—ç®€å•ä¾‹å­</code></pre>

    <h4>3. âœ… <strong>è¯­ä¹‰å‘é‡è¿ç®—</strong></h4>

    <pre><code>king - man + woman â‰ˆ queen

ç†è§£åŸç†ï¼šå‘é‡åŒ…å«è¯­ä¹‰ç‰¹å¾</code></pre>

    <h4>4. âœ… <strong>Word2Vec vs GloVe</strong></h4>

    <pre><code>Word2Vecï¼šä¸Šä¸‹æ–‡çª—å£ï¼Œå±€éƒ¨ä¿¡æ¯
GloVeï¼šå…±ç°çŸ©é˜µï¼Œå…¨å±€ç»Ÿè®¡</code></pre>

    <h4>5. âœ… <strong>Poolingæ–¹æ³•</strong></h4>

    <pre><code>Mean Poolingï¼šå¹³å‡å€¼
Max Poolingï¼šæœ€å¤§å€¼
Min Poolingï¼šæœ€å°å€¼

ç›®çš„ï¼šå°†è¯å‘é‡è½¬æ¢ä¸ºå¥å­å‘é‡</code></pre>

    <hr>

    <h3>å¯èƒ½çš„è€ƒé¢˜</h3>

    <h4>é¢˜å‹1ï¼šä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—</h4>

    <p><strong>é¢˜ç›®ï¼š</strong>è®¡ç®—ä»¥ä¸‹ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦</p>

    <pre><code>v1 = [3, 4]
v2 = [6, 8]</code></pre>

    <details>
        <summary>ç­”æ¡ˆ</summary>
        <pre><code>v1Â·v2 = 3Ã—6 + 4Ã—8 = 18 + 32 = 50

|v1| = âˆš(3Â² + 4Â²) = âˆš25 = 5
|v2| = âˆš(6Â² + 8Â²) = âˆš100 = 10

cos(Î¸) = 50 / (5 Ã— 10) = 50 / 50 = 1.0

è§£é‡Šï¼šå®Œå…¨ç›¸ä¼¼ï¼ˆv2 = 2Ã—v1ï¼Œæ–¹å‘ç›¸åŒï¼‰</code></pre>
    </details>

    <hr>

    <h4>é¢˜å‹2ï¼šMean Poolingè®¡ç®—</h4>

    <p><strong>é¢˜ç›®ï¼š</strong>ç»™å®šè¯å‘é‡ï¼Œè®¡ç®—å¥å­å‘é‡ï¼ˆmean poolingï¼‰</p>

    <pre><code>å¥å­ï¼š"cat sat"
cat = [1, 2, 3]
sat = [4, 5, 6]</code></pre>

    <details>
        <summary>ç­”æ¡ˆ</summary>
        <pre><code>mean = (cat + sat) / 2
     = ([1, 2, 3] + [4, 5, 6]) / 2
     = [5, 7, 9] / 2
     = [2.5, 3.5, 4.5]</code></pre>
    </details>

    <hr>

    <h4>é¢˜å‹3ï¼šç†è§£è¯­ä¹‰å‘ç®—</h4>

    <p><strong>é¢˜ç›®ï¼š</strong>è§£é‡Šä¸ºä»€ä¹ˆ <code>king - man + woman â‰ˆ queen</code></p>

    <details>
        <summary>ç­”æ¡ˆ</summary>
        <pre><code>kingçš„å‘é‡åŒ…å«ç‰¹å¾ï¼š
- royaltyï¼ˆç‹å®¤ï¼‰
- maleï¼ˆç”·æ€§ï¼‰
- powerï¼ˆæƒåŠ›ï¼‰

mançš„å‘é‡åŒ…å«ç‰¹å¾ï¼š
- maleï¼ˆç”·æ€§ï¼‰
- adultï¼ˆæˆäººï¼‰

womançš„å‘é‡åŒ…å«ç‰¹å¾ï¼š
- femaleï¼ˆå¥³æ€§ï¼‰
- adultï¼ˆæˆäººï¼‰

king - manï¼š
å»é™¤maleç‰¹å¾ï¼Œä¿ç•™royaltyå’Œpower

king - man + womanï¼š
åŠ ä¸Šfemaleç‰¹å¾

ç»“æœ = royalty + power + female â‰ˆ queen</code></pre>
    </details>

    <hr>

    <h4>é¢˜å‹4ï¼šé€‰æ‹©poolingæ–¹æ³•</h4>

    <p><strong>é¢˜ç›®ï¼š</strong>ä»¥ä¸‹åœºæ™¯åº”è¯¥ä½¿ç”¨å“ªç§poolingï¼Ÿ</p>

    <pre><code>åœºæ™¯1ï¼šè¯†åˆ«å¥å­ä¸­æ˜¯å¦åŒ…å«å¼ºçƒˆæƒ…æ„Ÿè¯
åœºæ™¯2ï¼šè®¡ç®—å¥å­çš„æ•´ä½“æƒ…æ„Ÿå€¾å‘</code></pre>

    <details>
        <summary>ç­”æ¡ˆ</summary>
        <pre><code>åœºæ™¯1ï¼šMax Pooling
ç†ç”±ï¼š
- åªè¦æœ‰ä¸€ä¸ªå¼ºçƒˆçš„æƒ…æ„Ÿè¯å°±åº”è¯¥è¢«è¯†åˆ«
- Max poolingä¼šä¿ç•™æœ€å¼ºçš„ç‰¹å¾
- ä¾‹å¦‚ï¼š"It was okay but terrible" â†’ terribleçš„å¼ºåº¦ä¼šè¢«ä¿ç•™

åœºæ™¯2ï¼šMean Pooling
ç†ç”±ï¼š
- éœ€è¦è€ƒè™‘æ‰€æœ‰è¯çš„ç»¼åˆå½±å“
- å¹³å‡å€¼èƒ½æ›´å¥½åœ°åæ˜ æ•´ä½“å€¾å‘
- ä¾‹å¦‚ï¼š"good good good bad" â†’ å¹³å‡ååå‘positive</code></pre>
    </details>

    <hr>

    <h4>é¢˜å‹5ï¼šWord2Vecå‚æ•°ç†è§£</h4>

    <p><strong>é¢˜ç›®ï¼š</strong>è§£é‡ŠWord2Vecå‚æ•°çš„å«ä¹‰</p>

    <pre><code>Word2Vec(vector_size=100, window=5, min_count=2)</code></pre>

    <details>
        <summary>ç­”æ¡ˆ</summary>
        <pre><code>vector_size=100ï¼š
- æ¯ä¸ªè¯ç”¨100ç»´å‘é‡è¡¨ç¤º
- å½±å“ï¼šç»´åº¦è¶Šé«˜ï¼Œèƒ½æ•æ‰æ›´å¤šç»†èŠ‚ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜

window=5ï¼š
- ä¸Šä¸‹æ–‡çª—å£å¤§å°ä¸º5
- è€ƒè™‘ç›®æ ‡è¯å‰åå„5ä¸ªè¯
- å½±å“ï¼šçª—å£è¶Šå¤§ï¼Œæ•æ‰æ›´å¹¿çš„è¯­ä¹‰å…³ç³»

min_count=2ï¼š
- æœ€å°è¯é¢‘é˜ˆå€¼ä¸º2
- åªè®­ç»ƒå‡ºç°2æ¬¡ä»¥ä¸Šçš„è¯
- å½±å“ï¼šè¿‡æ»¤ç”Ÿåƒ»è¯ï¼Œå‡å°‘å™ªå£°</code></pre>
    </details>

    <hr>

    <h2>å…«ã€ä»£ç è¦ç‚¹</h2>

    <h3>1. GensimåŸºç¡€</h3>

    <pre><code>from gensim.models import Word2Vec

# è®­ç»ƒ
model = Word2Vec(sentences, vector_size=100, window=5)

# è·å–å‘é‡
vec = model.wv['word']

# æ‰¾ç›¸ä¼¼è¯
similar = model.wv.most_similar('word', topn=10)

# è¯ç±»æ¯”
result = model.wv.most_similar(
    positive=['king', 'woman'],
    negative=['man']
)</code></pre>

    <hr>

    <h3>2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹</h3>

    <pre><code>import gensim.downloader as api

# åˆ—å‡ºå¯ç”¨æ¨¡å‹
print(list(api.info()['models'].keys()))

# åŠ è½½æ¨¡å‹
model = api.load('glove-twitter-25')</code></pre>

    <hr>

    <h3>3. Poolingå®ç°</h3>

    <pre><code># Mean pooling
sentence_vec = np.mean([model[w] for w in words], axis=0)

# Max pooling
sentence_vec = np.max([model[w] for w in words], axis=0)</code></pre>

    <hr>

    <h3>4. SVMåˆ†ç±»</h3>

    <pre><code>from sklearn.svm import SVC

# è®­ç»ƒ
clf = SVC(kernel='linear')
clf.fit(X_train, y_train)

# é¢„æµ‹
y_pred = clf.predict(X_test)

# è¯„ä¼°
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_pred)</code></pre>

    <hr>

    <h2>ä¹ã€è®°å¿†å£è¯€</h2>

    <h3>è¯åµŒå…¥</h3>
    <pre><code>è¯å‘é‡ä½ç»´ç¨ å¯†ï¼Œ
è¯­ä¹‰ç›¸ä¼¼æ•°å€¼è¿‘ï¼Œ
åˆ†å¸ƒå‡è¯´æ˜¯åŸºç¡€ï¼Œ
ä¸Šä¸‹æ–‡å®šè¯æ„ä¹‰ã€‚</code></pre>

    <h3>å‘é‡è¿ç®—</h3>
    <pre><code>ä½™å¼¦ç›¸ä¼¼çœ‹è§’åº¦ï¼Œ
ç‚¹ç§¯é™¤ä»¥æ¨¡é•¿ç§¯ï¼Œ
kingå‡manåŠ womanï¼Œ
è¯­ä¹‰è¿ç®—queenå‡ºç°ã€‚</code></pre>

    <h3>Pooling</h3>
    <pre><code>è¯å‘é‡å˜å¥å‘é‡ï¼Œ
å¹³å‡æœ€å¤§æœ€å°ä¸‰ï¼Œ
Meané€‚åˆçœ‹æ•´ä½“ï¼Œ
Maxæ•æ‰æœ€å¼ºç‚¹ã€‚</code></pre>

    <hr>

    <h2>åã€å¸¸è§é”™è¯¯</h2>

    <h3>âŒ é”™è¯¯1ï¼šå‘é‡ç»´åº¦ä¸åŒ¹é…</h3>

    <pre><code># é”™è¯¯
vec1 = model1.wv['word']  # 100ç»´
vec2 = model2.wv['word']  # 300ç»´
similarity = cosine_similarity(vec1, vec2)  # âŒ ç»´åº¦ä¸åŒ

# æ­£ç¡®
# ç¡®ä¿ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹
vec1 = model.wv['word1']
vec2 = model.wv['word2']
similarity = cosine_similarity(vec1, vec2)  # âœ…</code></pre>

    <hr>

    <h3>âŒ é”™è¯¯2ï¼šå¿˜è®°å¤„ç†æœªç™»å½•è¯ï¼ˆOOVï¼‰</h3>

    <pre><code># é”™è¯¯
sentence_vec = np.mean([model[w] for w in words])  # âŒ
# å¦‚æœæœ‰è¯ä¸åœ¨æ¨¡å‹ä¸­ï¼Œä¼šæŠ¥é”™

# æ­£ç¡®
vectors = []
for w in words:
    if w in model:  # æ£€æŸ¥è¯æ˜¯å¦åœ¨æ¨¡å‹ä¸­
        vectors.append(model[w])

if len(vectors) > 0:
    sentence_vec = np.mean(vectors, axis=0)
else:
    sentence_vec = np.zeros(model.vector_size)  # âœ…</code></pre>

    <hr>

    <h3>âŒ é”™è¯¯3ï¼šä½¿ç”¨é”™è¯¯çš„axis</h3>

    <pre><code># é”™è¯¯
vectors = [v1, v2, v3]  # shape: (3, 100)
mean = np.mean(vectors, axis=1)  # âŒ æ²¿ç€ç¬¬1ç»´æ±‚å¹³å‡
# ç»“æœshape: (3,) é”™è¯¯ï¼

# æ­£ç¡®
mean = np.mean(vectors, axis=0)  # âœ… æ²¿ç€ç¬¬0ç»´æ±‚å¹³å‡
# ç»“æœshape: (100,) æ­£ç¡®ï¼</code></pre>

    <hr>

    <p><strong>ç¥ä½ è€ƒè¯•é¡ºåˆ©ï¼</strong> ğŸ‰</p>

    <p><strong>Lecture 8æ ¸å¿ƒè¦ç‚¹ï¼š</strong></p>
    <ul>
        <li>è¯åµŒå…¥ï¼šä½ç»´ç¨ å¯†å‘é‡è¡¨ç¤ºè¯</li>
        <li>Word2Vecï¼šåŸºäºä¸Šä¸‹æ–‡è®­ç»ƒè¯å‘é‡</li>
        <li>GloVeï¼šåŸºäºå…¨å±€å…±ç°çŸ©é˜µ</li>
        <li>ä½™å¼¦ç›¸ä¼¼åº¦ï¼šè®¡ç®—å‘é‡ç›¸ä¼¼æ€§</li>
        <li>è¯­ä¹‰è¿ç®—ï¼šking - man + woman â‰ˆ queen</li>
        <li>Poolingï¼šå°†è¯å‘é‡åˆå¹¶ä¸ºå¥å­å‘é‡ï¼ˆmean/max/minï¼‰</li>
        <li>åº”ç”¨ï¼šç”¨è¯åµŒå…¥åšæ–‡æœ¬åˆ†ç±»</li>
    </ul>

    <p><strong>è®°ä½ï¼š</strong>ç†è§£è¯åµŒå…¥çš„åŸç†ï¼ä¼šè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ä¼šåšpoolingï¼</p>

</body>
</html>