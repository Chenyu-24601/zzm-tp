# Lecture 8 è¶…åŸºç¡€å¤ä¹ ç¬”è®° - è¯åµŒå…¥ï¼ˆWord Embeddingsï¼‰

## é›¶ã€å…ˆç†è§£é—®é¢˜ï¼ˆä»€ä¹ˆæ˜¯è¯åµŒå…¥ï¼Ÿï¼‰

### ä¼ ç»Ÿæ–¹æ³•çš„é—®é¢˜

**One-hotç¼–ç ï¼š**

```
è¯æ±‡è¡¨ï¼š['cat', 'dog', 'king', 'queen']

cat   = [1, 0, 0, 0]
dog   = [0, 1, 0, 0]
king  = [0, 0, 1, 0]
queen = [0, 0, 0, 1]
```

**é—®é¢˜ï¼š**

```
1. ç»´åº¦å¤ªé«˜
   10ä¸‡ä¸ªè¯ â†’ 10ä¸‡ç»´å‘é‡

2. æ— æ³•è¡¨ç¤ºè¯­ä¹‰å…³ç³»
   catå’Œdogéƒ½æ˜¯åŠ¨ç‰©ï¼Œä½†å‘é‡å®Œå…¨ä¸ç›¸å…³
   kingå’Œqueenéƒ½æ˜¯royaltyï¼Œä½†å‘é‡ä¹Ÿå®Œå…¨ä¸ç›¸å…³

3. ç¨€ç–æ€§
   å‘é‡ä¸­åªæœ‰ä¸€ä¸ª1ï¼Œå…¶ä½™éƒ½æ˜¯0
```

---

### è¯åµŒå…¥çš„è§£å†³æ–¹æ¡ˆ

**å®šä¹‰ï¼š**å°†è¯æ˜ å°„åˆ°ä½ç»´ç¨ å¯†å‘é‡

**ä¾‹å­ï¼š**

```
# 3ç»´è¯åµŒå…¥ï¼ˆå®é™…é€šå¸¸100-300ç»´ï¼‰
cat   = [0.2, 0.8, 0.1]  â† åŠ¨ç‰©ç‰¹å¾å¼º
dog   = [0.3, 0.7, 0.2]  â† ä¹Ÿæ˜¯åŠ¨ç‰©ï¼Œå‘é‡ç›¸ä¼¼
king  = [0.9, 0.1, 0.8]  â† royaltyç‰¹å¾
queen = [0.8, 0.2, 0.9]  â† ä¹Ÿæ˜¯royaltyï¼Œå‘é‡ç›¸ä¼¼
```

**ä¼˜ç‚¹ï¼š**

```
1. ä½ç»´åº¦
   10ä¸‡ä¸ªè¯ â†’ 300ç»´å‘é‡ï¼ˆå‡å°‘333å€ï¼‰

2. è¯­ä¹‰ç›¸ä¼¼
   ç›¸ä¼¼çš„è¯æœ‰ç›¸ä¼¼çš„å‘é‡
   cat â‰ˆ dogï¼ˆéƒ½æ˜¯åŠ¨ç‰©ï¼‰
   king â‰ˆ queenï¼ˆéƒ½æ˜¯royaltyï¼‰

3. ç¨ å¯†
   å‘é‡ä¸­å¤§éƒ¨åˆ†å…ƒç´ éƒ½æœ‰æ„ä¹‰
```

---

## ä¸€ã€Word2VecåŸç†

### 1. æ ¸å¿ƒæ€æƒ³

**åˆ†å¸ƒå‡è¯´ï¼ˆDistributional Hypothesisï¼‰ï¼š**

```
"You shall know a word by the company it keeps"
ï¼ˆé€šè¿‡ä¸€ä¸ªè¯çš„é‚»å±…æ¥äº†è§£å®ƒï¼‰

ä¾‹å­ï¼š
"The cat sat on the mat"
"The dog sat on the rug"

catå’Œdogå‡ºç°åœ¨ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­
â†’ å®ƒä»¬åº”è¯¥æœ‰ç›¸ä¼¼çš„æ„ä¹‰
â†’ å®ƒä»¬åº”è¯¥æœ‰ç›¸ä¼¼çš„å‘é‡
```

---

### 2. Word2Vecçš„ä¸¤ç§æ¨¡å‹

#### CBOWï¼ˆContinuous Bag of Wordsï¼‰

**ç›®æ ‡ï¼š**ç”¨ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯

```
è¾“å…¥ï¼šä¸Šä¸‹æ–‡è¯
è¾“å‡ºï¼šä¸­å¿ƒè¯

ä¾‹å­ï¼š
ä¸Šä¸‹æ–‡ï¼š["The", "cat", "on", "the", "mat"]
é¢„æµ‹ï¼šsat

ä¸Šä¸‹æ–‡ï¼š["The", "dog", "on", "the", "rug"]
é¢„æµ‹ï¼šsat
```

---

#### Skip-gram

**ç›®æ ‡ï¼š**ç”¨ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡

```
è¾“å…¥ï¼šä¸­å¿ƒè¯
è¾“å‡ºï¼šä¸Šä¸‹æ–‡è¯

ä¾‹å­ï¼š
è¾“å…¥ï¼šsat
é¢„æµ‹ï¼š["The", "cat", "on", "the", "mat"]

è¾“å…¥ï¼šsat
é¢„æµ‹ï¼š["The", "dog", "on", "the", "rug"]
```

---

### 3. è®­ç»ƒè¿‡ç¨‹ï¼ˆç®€åŒ–ï¼‰

```
æ­¥éª¤1ï¼šéšæœºåˆå§‹åŒ–è¯å‘é‡
cat   = [random, random, random]
dog   = [random, random, random]
...

æ­¥éª¤2ï¼šéå†è®­ç»ƒæ–‡æœ¬
å¯¹äºå¥å­ï¼š"The cat sat on the mat"
  å¯¹äºæ¯ä¸ªè¯åŠå…¶ä¸Šä¸‹æ–‡
    è°ƒæ•´å‘é‡ï¼Œä½¿å¾—ï¼š
    - ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„è¯ï¼Œå‘é‡æ›´æ¥è¿‘
    - ä¸Šä¸‹æ–‡ä¸­ä¸å‡ºç°çš„è¯ï¼Œå‘é‡æ›´è¿œç¦»

æ­¥éª¤3ï¼šè¿­ä»£å¤šæ¬¡ï¼Œç›´åˆ°æ”¶æ•›

ç»“æœï¼š
catå’Œdogçš„å‘é‡å˜å¾—ç›¸ä¼¼ï¼ˆå› ä¸ºä¸Šä¸‹æ–‡ç›¸ä¼¼ï¼‰
```

---

## äºŒã€å‘é‡è¿ç®—

### 1. ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰

**å…¬å¼ï¼š**

```
cos(Î¸) = (A Â· B) / (|A| Ã— |B|)

å…¶ä¸­ï¼š
A Â· B = Aå’ŒBçš„ç‚¹ç§¯
|A| = Açš„æ¨¡é•¿
|B| = Bçš„æ¨¡é•¿
```

**æ‰‹å·¥è®¡ç®—ç¤ºä¾‹ï¼š**

```
A = [1, 2, 3]
B = [4, 5, 6]

A Â· B = 1Ã—4 + 2Ã—5 + 3Ã—6 = 4 + 10 + 18 = 32

|A| = âˆš(1Â² + 2Â² + 3Â²) = âˆš14 â‰ˆ 3.74
|B| = âˆš(4Â² + 5Â² + 6Â²) = âˆš77 â‰ˆ 8.77

cos(Î¸) = 32 / (3.74 Ã— 8.77) = 32 / 32.8 â‰ˆ 0.976
```

**è§£é‡Šï¼š**

```
cos(Î¸) â‰ˆ 1  â†’ å‘é‡éå¸¸ç›¸ä¼¼ï¼ˆå¤¹è§’æ¥è¿‘0Â°ï¼‰
cos(Î¸) â‰ˆ 0  â†’ å‘é‡ä¸ç›¸å…³ï¼ˆå¤¹è§’æ¥è¿‘90Â°ï¼‰
cos(Î¸) â‰ˆ -1 â†’ å‘é‡ç›¸åï¼ˆå¤¹è§’æ¥è¿‘180Â°ï¼‰
```

**ä»£ç å®ç°ï¼š**

```python
import numpy as np

def cosine_similarity(v1, v2):
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    # ç‚¹ç§¯
    dot_product = np.dot(v1, v2)

    # æ¨¡é•¿
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)

    # ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = dot_product / (norm_v1 * norm_v2)

    return similarity

# ä¾‹å­
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])
print(cosine_similarity(v1, v2))  # 0.974
```

---

### 2. è¯­ä¹‰å‘é‡è¿ç®—

**è‘—åä¾‹å­ï¼š**

```
king - man + woman â‰ˆ queen
```

**ä¸ºä»€ä¹ˆå¯è¡Œï¼Ÿ**

```
kingçš„å‘é‡åŒ…å«ï¼š
  - royaltyï¼ˆç‹å®¤ï¼‰
  - maleï¼ˆç”·æ€§ï¼‰
  - powerï¼ˆæƒåŠ›ï¼‰

mançš„å‘é‡åŒ…å«ï¼š
  - maleï¼ˆç”·æ€§ï¼‰
  - humanï¼ˆäººç±»ï¼‰

womançš„å‘é‡åŒ…å«ï¼š
  - femaleï¼ˆå¥³æ€§ï¼‰
  - humanï¼ˆäººç±»ï¼‰

king - manï¼šå»é™¤"ç”·æ€§"ç‰¹å¾ï¼Œä¿ç•™"ç‹å®¤+æƒåŠ›"
king - man + womanï¼šåŠ ä¸Š"å¥³æ€§"ç‰¹å¾
ç»“æœï¼šroyalty + female = queen âœ“
```

**å…¶ä»–ä¾‹å­ï¼š**

```
Paris - France + Italy â‰ˆ Rome
ï¼ˆé¦–éƒ½å…³ç³»ï¼‰

bigger - big + small â‰ˆ smaller
ï¼ˆæ¯”è¾ƒçº§å…³ç³»ï¼‰

walking - walk + swim â‰ˆ swimming
ï¼ˆåŠ¨åè¯å…³ç³»ï¼‰
```

---

## ä¸‰ã€ä½¿ç”¨Gensimè®­ç»ƒWord2Vec

### 1. å‡†å¤‡æ•°æ®

```python
import nltk
from nltk.corpus import gutenberg

# ä¸‹è½½Moby Dick
nltk.download('gutenberg')

# è¯»å–æ–‡æœ¬
moby_dick = gutenberg.sents('melville-moby_dick.txt')

# æ•°æ®æ ¼å¼ï¼š
# [['Call', 'me', 'Ishmael', '.'],
#  ['Some', 'years', 'ago', ...],
#  ...]
```

---

### 2. è®­ç»ƒæ¨¡å‹

```python
from gensim.models import Word2Vec

# è®­ç»ƒæ¨¡å‹ï¼ˆä¸€è¡Œä»£ç ï¼ï¼‰
model = Word2Vec(
    sentences=moby_dick,  # è®­ç»ƒæ•°æ®
    vector_size=100,      # å‘é‡ç»´åº¦
    window=5,             # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    min_count=5,          # æœ€å°è¯é¢‘ï¼ˆä½äºæ­¤å€¼çš„è¯è¢«å¿½ç•¥ï¼‰
    workers=4             # å¹¶è¡Œçº¿ç¨‹æ•°
)
```

**å‚æ•°è§£é‡Šï¼š**

```
vector_size=100ï¼š
  æ¯ä¸ªè¯ç”¨100ç»´å‘é‡è¡¨ç¤º

window=5ï¼š
  è€ƒè™‘å‰å5ä¸ªè¯ä½œä¸ºä¸Šä¸‹æ–‡
  ä¾‹å¦‚ï¼š"The cat sat on the mat"
  å¯¹äº"sat"ï¼Œä¸Šä¸‹æ–‡æ˜¯["The", "cat", "on", "the", "mat"]

min_count=5ï¼š
  åªè®­ç»ƒå‡ºç°5æ¬¡ä»¥ä¸Šçš„è¯
  è¿‡æ»¤æ‰ç”Ÿåƒ»è¯å’Œæ‹¼å†™é”™è¯¯

workers=4ï¼š
  ä½¿ç”¨4ä¸ªCPUæ ¸å¿ƒå¹¶è¡Œè®­ç»ƒ
  åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹
```

---

### 3. ä½¿ç”¨æ¨¡å‹

#### è·å–è¯å‘é‡

```python
# è·å–å•è¯çš„å‘é‡
whale_vec = model.wv['whale']
print(whale_vec.shape)  # (100,)
print(whale_vec[:5])    # [0.123, -0.456, 0.789, ...]
```

---

#### æŸ¥æ‰¾ç›¸ä¼¼è¯

```python
# æ‰¾æœ€ç›¸ä¼¼çš„10ä¸ªè¯
similar_words = model.wv.most_similar('whale', topn=10)
print(similar_words)

# è¾“å‡ºç¤ºä¾‹ï¼š
# [('ship', 0.95),
#  ('boat', 0.93),
#  ('sea', 0.91),
#  ('ocean', 0.89),
#  ...]
```

---

#### è¯è¯­ç±»æ¯”

```python
# king - man + woman â‰ˆ queen
result = model.wv.most_similar(
    positive=['king', 'woman'],  # åŠ ä¸Šè¿™äº›è¯
    negative=['man'],            # å‡å»è¿™äº›è¯
    topn=1
)
print(result)  # [('queen', 0.85)]
```

---

### 4. å®ç°most_similaræ–¹æ³•

**ä»»åŠ¡ï¼š**è‡ªå·±å®ç°ç±»ä¼¼Gensimçš„most_similaråŠŸèƒ½

```python
def most_similar(word, model, topn=10):
    """
    æ‰¾å‡ºä¸ç»™å®šè¯æœ€ç›¸ä¼¼çš„topnä¸ªè¯

    å‚æ•°ï¼š
    wordï¼šç›®æ ‡è¯
    modelï¼šè®­ç»ƒå¥½çš„Word2Vecæ¨¡å‹
    topnï¼šè¿”å›å‰nä¸ªç»“æœ

    è¿”å›ï¼š
    [(word1, similarity1), (word2, similarity2), ...]
    """
    # è·å–ç›®æ ‡è¯çš„å‘é‡
    target_vec = model.wv[word]

    # å­˜å‚¨æ‰€æœ‰è¯çš„ç›¸ä¼¼åº¦
    similarities = []

    # éå†è¯æ±‡è¡¨ä¸­çš„æ‰€æœ‰è¯
    for vocab_word in model.wv.index_to_key:
        # è·³è¿‡ç›®æ ‡è¯æœ¬èº«
        if vocab_word == word:
            continue

        # è·å–å½“å‰è¯çš„å‘é‡
        current_vec = model.wv[vocab_word]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sim = cosine_similarity(target_vec, current_vec)

        # å­˜å‚¨
        similarities.append((vocab_word, sim))

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)

    # è¿”å›å‰topnä¸ª
    return similarities[:topn]

# ä½¿ç”¨
result = most_similar('ship', model, topn=5)
print(result)
# [('boat', 0.9945),
#  ('head', 0.9881),
#  ('boiling', 0.9821),
#  ...]
```

---

## å››ã€GloVeæ¨¡å‹

### 1. GloVe vs Word2Vec

**Word2Vecï¼š**
```
åŸºäºä¸Šä¸‹æ–‡çª—å£
å±€éƒ¨ä¿¡æ¯

ä¾‹å¦‚ï¼šåœ¨"The cat sat"ä¸­
åªçœ‹åˆ°catå’Œsatçš„å±€éƒ¨å…±ç°
```

**GloVeï¼š**
```
åŸºäºå…¨å±€å…±ç°çŸ©é˜µ
å…¨å±€ç»Ÿè®¡ä¿¡æ¯

ç»Ÿè®¡æ•´ä¸ªè¯­æ–™åº“ä¸­
catå’Œsatå…±ç°äº†å¤šå°‘æ¬¡
```

---

### 2. åŠ è½½é¢„è®­ç»ƒGloVeæ¨¡å‹

```python
import gensim.downloader as api

# ä¸‹è½½é¢„è®­ç»ƒçš„GloVeæ¨¡å‹ï¼ˆTwitteræ•°æ®ï¼‰
# è¿™ä¼šä¸‹è½½å¤§çº¦1GBçš„æ–‡ä»¶
glove_model = api.load('glove-twitter-25')

# ä½¿ç”¨æ–¹æ³•å’ŒWord2Vecç›¸åŒ
similar_words = glove_model.most_similar('happy', topn=5)
print(similar_words)
# [('glad', 0.89),
#  ('pleased', 0.87),
#  ('excited', 0.85),
#  ...]
```

**é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜ç‚¹ï¼š**

```
1. æ— éœ€è‡ªå·±è®­ç»ƒ
   â†’ èŠ‚çœæ—¶é—´å’Œè®¡ç®—èµ„æº

2. è®­ç»ƒæ•°æ®é‡å¤§
   â†’ 20äº¿Twitteræ¨æ–‡
   â†’ è¦†ç›–æ›´å¤šè¯æ±‡

3. è´¨é‡é«˜
   â†’ ä¸“ä¸šå›¢é˜Ÿè®­ç»ƒ
   â†’ å‚æ•°ä¼˜åŒ–å¥½
```

---

## äº”ã€æ–‡æœ¬åˆ†ç±»åº”ç”¨

### é—®é¢˜ï¼šå¦‚ä½•ç”¨è¯å‘é‡åˆ†ç±»å¥å­ï¼Ÿ

**æŒ‘æˆ˜ï¼š**

```
æœ‰è¯å‘é‡ï¼š
cat   = [0.2, 0.8, 0.1]
dog   = [0.3, 0.7, 0.2]
sat   = [0.5, 0.5, 0.5]

ä½†å¥å­æ€ä¹ˆè¡¨ç¤ºï¼Ÿ
"The cat sat" = ?
```

---

### è§£å†³æ–¹æ¡ˆï¼šPoolingï¼ˆæ± åŒ–ï¼‰

#### Mean Poolingï¼ˆå¹³å‡æ± åŒ–ï¼‰

**æ–¹æ³•ï¼š**å¯¹æ‰€æœ‰è¯å‘é‡æ±‚å¹³å‡

```python
def mean_pooling(sentence, model):
    """
    è®¡ç®—å¥å­çš„å¹³å‡å‘é‡

    å‚æ•°ï¼š
    sentenceï¼šå¥å­ï¼ˆè¯åˆ—è¡¨ï¼‰
    modelï¼šè¯å‘é‡æ¨¡å‹

    è¿”å›ï¼š
    å¥å­å‘é‡
    """
    # å­˜å‚¨æ‰€æœ‰è¯çš„å‘é‡
    vectors = []

    for word in sentence:
        # å¦‚æœè¯åœ¨æ¨¡å‹ä¸­ï¼Œè·å–å…¶å‘é‡
        if word in model:
            vectors.append(model[word])

    # å¦‚æœæ²¡æœ‰è¯åœ¨æ¨¡å‹ä¸­ï¼Œè¿”å›é›¶å‘é‡
    if len(vectors) == 0:
        return np.zeros(model.vector_size)

    # è®¡ç®—å¹³å‡å€¼
    sentence_vec = np.mean(vectors, axis=0)

    return sentence_vec

# ä¾‹å­
sentence = ['the', 'cat', 'sat']
sentence_vec = mean_pooling(sentence, glove_model)
print(sentence_vec.shape)  # (25,) å¦‚æœç”¨25ç»´GloVe
```

**æ‰‹å·¥è®¡ç®—ç¤ºä¾‹ï¼š**

```
å‡è®¾3ç»´å‘é‡ï¼š
the = [0.1, 0.2, 0.3]
cat = [0.4, 0.5, 0.6]
sat = [0.7, 0.8, 0.9]

mean = (the + cat + sat) / 3
     = ([0.1, 0.2, 0.3] + [0.4, 0.5, 0.6] + [0.7, 0.8, 0.9]) / 3
     = [1.2, 1.5, 1.8] / 3
     = [0.4, 0.5, 0.6]
```

---

#### Max Poolingï¼ˆæœ€å¤§æ± åŒ–ï¼‰

```python
def max_pooling(sentence, model):
    """æœ€å¤§æ± åŒ–ï¼šå–æ¯ä¸ªç»´åº¦çš„æœ€å¤§å€¼"""
    vectors = []
    for word in sentence:
        if word in model:
            vectors.append(model[word])

    if len(vectors) == 0:
        return np.zeros(model.vector_size)

    sentence_vec = np.max(vectors, axis=0)
    return sentence_vec
```

**æ‰‹å·¥è®¡ç®—ç¤ºä¾‹ï¼š**

```
the = [0.1, 0.2, 0.3]
cat = [0.4, 0.5, 0.6]
sat = [0.7, 0.8, 0.9]

max = [max(0.1, 0.4, 0.7),
       max(0.2, 0.5, 0.8),
       max(0.3, 0.6, 0.9)]
    = [0.7, 0.8, 0.9]
```

---

### æƒ…æ„Ÿåˆ†ç±»å®Œæ•´æµç¨‹

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
df = pd.read_csv('Tweets_short.csv')

# æ­¥éª¤2ï¼šé¢„å¤„ç†
def preprocess_tweet(text):
    """ç®€å•çš„é¢„å¤„ç†"""
    # è½¬å°å†™
    text = text.lower()
    # åˆ†è¯
    tokens = text.split()
    return tokens

df['tokens'] = df['text'].apply(preprocess_tweet)

# æ­¥éª¤3ï¼šæå–å¥å­å‘é‡
df['vector'] = df['tokens'].apply(
    lambda tokens: mean_pooling(tokens, glove_model)
)

# æ­¥éª¤4ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®
X = np.vstack(df['vector'].values)  # ç‰¹å¾çŸ©é˜µ
y = df['sentiment'].values          # æ ‡ç­¾

# æ­¥éª¤5ï¼šåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# æ­¥éª¤6ï¼šè®­ç»ƒåˆ†ç±»å™¨
classifier = SVC(kernel='linear')
classifier.fit(X_train, y_train)

# æ­¥éª¤7ï¼šé¢„æµ‹
y_pred = classifier.predict(X_test)

# æ­¥éª¤8ï¼šè¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2%}')

# æ‰“å°è¯¦ç»†æŠ¥å‘Š
print(classification_report(y_test, y_pred))
```

---

### é¢„å¤„ç†Twitterç‰¹å¾

**é—®é¢˜ï¼š**Twitteræ–‡æœ¬æœ‰ç‰¹æ®Šå…ƒç´ 

```
"@user I love #python! Check this out: http://..."
```

**GloVeæ¨¡å‹çš„é¢„å¤„ç†ï¼š**

```
@user â†’ <USER>
#python â†’ <HASHTAG>
http://... â†’ <URL>
```

**å®ç°ï¼š**

```python
import re

def preprocess_tweet_glove(text):
    """
    æŒ‰ç…§GloVeæ¨¡å‹çš„é¢„å¤„ç†æ–¹å¼å¤„ç†Twitteræ–‡æœ¬
    """
    # æ›¿æ¢URL
    text = re.sub(r'https?://\S+', '<URL>', text)

    # æ›¿æ¢@mentions
    text = re.sub(r'@\w+', '<USER>', text)

    # æ›¿æ¢#hashtags
    text = re.sub(r'#(\w+)', '<HASHTAG>', text)

    # è½¬å°å†™
    text = text.lower()

    # åˆ†è¯
    tokens = text.split()

    return tokens

# ä¾‹å­
text = "@john I love #python! Check http://example.com"
print(preprocess_tweet_glove(text))
# ['<USER>', 'i', 'love', '<HASHTAG>', '!', 'check', '<URL>']
```

---

## å…­ã€é”™è¯¯åˆ†æ

### æ··æ·†çŸ©é˜µ

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# è®¡ç®—æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred,
                      labels=['positive', 'neutral', 'negative'])

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d',
            xticklabels=['positive', 'neutral', 'negative'],
            yticklabels=['positive', 'neutral', 'negative'])
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()
```

---

### åˆ†æé”™è¯¯æ ·æœ¬

```python
# æ‰¾å‡ºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬
df_test = df.loc[X_test.index]
df_test['predicted'] = y_pred
df_test['correct'] = (y_test == y_pred)

# é”™è¯¯æ ·æœ¬
errors = df_test[df_test['correct'] == False]

# åˆ†æï¼šnegativeè¢«é¢„æµ‹ä¸ºpositive
neg_as_pos = errors[(errors['sentiment'] == 'negative') &
                    (errors['predicted'] == 'positive')]

print(f"Negativeé¢„æµ‹ä¸ºPositiveçš„æ ·æœ¬æ•°ï¼š{len(neg_as_pos)}")
print("\nç¤ºä¾‹ï¼š")
for idx, row in neg_as_pos.head(5).iterrows():
    print(f"æ–‡æœ¬ï¼š{row['text']}")
    print(f"çœŸå®ï¼š{row['sentiment']}, é¢„æµ‹ï¼š{row['predicted']}\n")
```

---

## ä¸ƒã€çº¸ç¬”è€ƒè¯•é‡ç‚¹

### å¿…é¡»æŒæ¡

#### 1. âœ… **è¯åµŒå…¥çš„å®šä¹‰**

```
å®šä¹‰ï¼šå°†è¯æ˜ å°„åˆ°ä½ç»´ç¨ å¯†å‘é‡
ç›®çš„ï¼šæ•æ‰è¯çš„è¯­ä¹‰ä¿¡æ¯
```

#### 2. âœ… **ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—**

```
å…¬å¼ï¼šcos(Î¸) = (AÂ·B) / (|A|Ã—|B|)

ä¼šæ‰‹å·¥è®¡ç®—ç®€å•ä¾‹å­
```

#### 3. âœ… **è¯­ä¹‰å‘é‡è¿ç®—**

```
king - man + woman â‰ˆ queen

ç†è§£åŸç†ï¼šå‘é‡åŒ…å«è¯­ä¹‰ç‰¹å¾
```

#### 4. âœ… **Word2Vec vs GloVe**

```
Word2Vecï¼šä¸Šä¸‹æ–‡çª—å£ï¼Œå±€éƒ¨ä¿¡æ¯
GloVeï¼šå…±ç°çŸ©é˜µï¼Œå…¨å±€ç»Ÿè®¡
```

#### 5. âœ… **Poolingæ–¹æ³•**

```
Mean Poolingï¼šå¹³å‡å€¼
Max Poolingï¼šæœ€å¤§å€¼
Min Poolingï¼šæœ€å°å€¼

ç›®çš„ï¼šå°†è¯å‘é‡è½¬æ¢ä¸ºå¥å­å‘é‡
```

---

### å¯èƒ½çš„è€ƒé¢˜

#### é¢˜å‹1ï¼šä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

**é¢˜ç›®ï¼š**è®¡ç®—ä»¥ä¸‹ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

```
v1 = [3, 4]
v2 = [6, 8]
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
v1Â·v2 = 3Ã—6 + 4Ã—8 = 18 + 32 = 50

|v1| = âˆš(3Â² + 4Â²) = âˆš25 = 5
|v2| = âˆš(6Â² + 8Â²) = âˆš100 = 10

cos(Î¸) = 50 / (5 Ã— 10) = 50 / 50 = 1.0

è§£é‡Šï¼šå®Œå…¨ç›¸ä¼¼ï¼ˆv2 = 2Ã—v1ï¼Œæ–¹å‘ç›¸åŒï¼‰
```
</details>

---

#### é¢˜å‹2ï¼šMean Poolingè®¡ç®—

**é¢˜ç›®ï¼š**ç»™å®šè¯å‘é‡ï¼Œè®¡ç®—å¥å­å‘é‡ï¼ˆmean poolingï¼‰

```
å¥å­ï¼š"cat sat"
cat = [1, 2, 3]
sat = [4, 5, 6]
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
mean = (cat + sat) / 2
     = ([1, 2, 3] + [4, 5, 6]) / 2
     = [5, 7, 9] / 2
     = [2.5, 3.5, 4.5]
```
</details>

---

#### é¢˜å‹3ï¼šç†è§£è¯­ä¹‰å‘ç®—

**é¢˜ç›®ï¼š**è§£é‡Šä¸ºä»€ä¹ˆ `king - man + woman â‰ˆ queen`

<details>
<summary>ç­”æ¡ˆ</summary>

```
kingçš„å‘é‡åŒ…å«ç‰¹å¾ï¼š
- royaltyï¼ˆç‹å®¤ï¼‰
- maleï¼ˆç”·æ€§ï¼‰
- powerï¼ˆæƒåŠ›ï¼‰

mançš„å‘é‡åŒ…å«ç‰¹å¾ï¼š
- maleï¼ˆç”·æ€§ï¼‰
- adultï¼ˆæˆäººï¼‰

womançš„å‘é‡åŒ…å«ç‰¹å¾ï¼š
- femaleï¼ˆå¥³æ€§ï¼‰
- adultï¼ˆæˆäººï¼‰

king - manï¼š
å»é™¤maleç‰¹å¾ï¼Œä¿ç•™royaltyå’Œpower

king - man + womanï¼š
åŠ ä¸Šfemaleç‰¹å¾

ç»“æœ = royalty + power + female â‰ˆ queen
```
</details>

---

#### é¢˜å‹4ï¼šé€‰æ‹©poolingæ–¹æ³•

**é¢˜ç›®ï¼š**ä»¥ä¸‹åœºæ™¯åº”è¯¥ä½¿ç”¨å“ªç§poolingï¼Ÿ

```
åœºæ™¯1ï¼šè¯†åˆ«å¥å­ä¸­æ˜¯å¦åŒ…å«å¼ºçƒˆæƒ…æ„Ÿè¯
åœºæ™¯2ï¼šè®¡ç®—å¥å­çš„æ•´ä½“æƒ…æ„Ÿå€¾å‘
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
åœºæ™¯1ï¼šMax Pooling
ç†ç”±ï¼š
- åªè¦æœ‰ä¸€ä¸ªå¼ºçƒˆçš„æƒ…æ„Ÿè¯å°±åº”è¯¥è¢«è¯†åˆ«
- Max poolingä¼šä¿ç•™æœ€å¼ºçš„ç‰¹å¾
- ä¾‹å¦‚ï¼š"It was okay but terrible" â†’ terribleçš„å¼ºåº¦ä¼šè¢«ä¿ç•™

åœºæ™¯2ï¼šMean Pooling
ç†ç”±ï¼š
- éœ€è¦è€ƒè™‘æ‰€æœ‰è¯çš„ç»¼åˆå½±å“
- å¹³å‡å€¼èƒ½æ›´å¥½åœ°åæ˜ æ•´ä½“å€¾å‘
- ä¾‹å¦‚ï¼š"good good good bad" â†’ å¹³å‡ååå‘positive
```
</details>

---

#### é¢˜å‹5ï¼šWord2Vecå‚æ•°ç†è§£

**é¢˜ç›®ï¼š**è§£é‡ŠWord2Vecå‚æ•°çš„å«ä¹‰

```python
Word2Vec(vector_size=100, window=5, min_count=2)
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
vector_size=100ï¼š
- æ¯ä¸ªè¯ç”¨100ç»´å‘é‡è¡¨ç¤º
- å½±å“ï¼šç»´åº¦è¶Šé«˜ï¼Œèƒ½æ•æ‰æ›´å¤šç»†èŠ‚ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜

window=5ï¼š
- ä¸Šä¸‹æ–‡çª—å£å¤§å°ä¸º5
- è€ƒè™‘ç›®æ ‡è¯å‰åå„5ä¸ªè¯
- å½±å“ï¼šçª—å£è¶Šå¤§ï¼Œæ•æ‰æ›´å¹¿çš„è¯­ä¹‰å…³ç³»

min_count=2ï¼š
- æœ€å°è¯é¢‘é˜ˆå€¼ä¸º2
- åªè®­ç»ƒå‡ºç°2æ¬¡ä»¥ä¸Šçš„è¯
- å½±å“ï¼šè¿‡æ»¤ç”Ÿåƒ»è¯ï¼Œå‡å°‘å™ªå£°
```
</details>

---

## å…«ã€ä»£ç è¦ç‚¹

### 1. GensimåŸºç¡€

```python
from gensim.models import Word2Vec

# è®­ç»ƒ
model = Word2Vec(sentences, vector_size=100, window=5)

# è·å–å‘é‡
vec = model.wv['word']

# æ‰¾ç›¸ä¼¼è¯
similar = model.wv.most_similar('word', topn=10)

# è¯ç±»æ¯”
result = model.wv.most_similar(
    positive=['king', 'woman'],
    negative=['man']
)
```

---

### 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
import gensim.downloader as api

# åˆ—å‡ºå¯ç”¨æ¨¡å‹
print(list(api.info()['models'].keys()))

# åŠ è½½æ¨¡å‹
model = api.load('glove-twitter-25')
```

---

### 3. Poolingå®ç°

```python
# Mean pooling
sentence_vec = np.mean([model[w] for w in words], axis=0)

# Max pooling
sentence_vec = np.max([model[w] for w in words], axis=0)
```

---

### 4. SVMåˆ†ç±»

```python
from sklearn.svm import SVC

# è®­ç»ƒ
clf = SVC(kernel='linear')
clf.fit(X_train, y_train)

# é¢„æµ‹
y_pred = clf.predict(X_test)

# è¯„ä¼°
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_pred)
```

---

## ä¹ã€è®°å¿†å£è¯€

### è¯åµŒå…¥
```
è¯å‘é‡ä½ç»´ç¨ å¯†ï¼Œ
è¯­ä¹‰ç›¸ä¼¼æ•°å€¼è¿‘ï¼Œ
åˆ†å¸ƒå‡è¯´æ˜¯åŸºç¡€ï¼Œ
ä¸Šä¸‹æ–‡å®šè¯æ„ä¹‰ã€‚
```

### å‘é‡è¿ç®—
```
ä½™å¼¦ç›¸ä¼¼çœ‹è§’åº¦ï¼Œ
ç‚¹ç§¯é™¤ä»¥æ¨¡é•¿ç§¯ï¼Œ
kingå‡manåŠ womanï¼Œ
è¯­ä¹‰è¿ç®—queenå‡ºç°ã€‚
```

### Pooling
```
è¯å‘é‡å˜å¥å‘é‡ï¼Œ
å¹³å‡æœ€å¤§æœ€å°ä¸‰ï¼Œ
Meané€‚åˆçœ‹æ•´ä½“ï¼Œ
Maxæ•æ‰æœ€å¼ºç‚¹ã€‚
```

---

## åã€å¸¸è§é”™è¯¯

### âŒ é”™è¯¯1ï¼šå‘é‡ç»´åº¦ä¸åŒ¹é…

```python
# é”™è¯¯
vec1 = model1.wv['word']  # 100ç»´
vec2 = model2.wv['word']  # 300ç»´
similarity = cosine_similarity(vec1, vec2)  # âŒ ç»´åº¦ä¸åŒ

# æ­£ç¡®
# ç¡®ä¿ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹
vec1 = model.wv['word1']
vec2 = model.wv['word2']
similarity = cosine_similarity(vec1, vec2)  # âœ…
```

---

### âŒ é”™è¯¯2ï¼šå¿˜è®°å¤„ç†æœªç™»å½•è¯ï¼ˆOOVï¼‰

```python
# é”™è¯¯
sentence_vec = np.mean([model[w] for w in words])  # âŒ
# å¦‚æœæœ‰è¯ä¸åœ¨æ¨¡å‹ä¸­ï¼Œä¼šæŠ¥é”™

# æ­£ç¡®
vectors = []
for w in words:
    if w in model:  # æ£€æŸ¥è¯æ˜¯å¦åœ¨æ¨¡å‹ä¸­
        vectors.append(model[w])

if len(vectors) > 0:
    sentence_vec = np.mean(vectors, axis=0)
else:
    sentence_vec = np.zeros(model.vector_size)  # âœ…
```

---

### âŒ é”™è¯¯3ï¼šä½¿ç”¨é”™è¯¯çš„axis

```python
# é”™è¯¯
vectors = [v1, v2, v3]  # shape: (3, 100)
mean = np.mean(vectors, axis=1)  # âŒ æ²¿ç€ç¬¬1ç»´æ±‚å¹³å‡
# ç»“æœshape: (3,) é”™è¯¯ï¼

# æ­£ç¡®
mean = np.mean(vectors, axis=0)  # âœ… æ²¿ç€ç¬¬0ç»´æ±‚å¹³å‡
# ç»“æœshape: (100,) æ­£ç¡®ï¼
```

---

**ç¥ä½ è€ƒè¯•é¡ºåˆ©ï¼** ğŸ‰

**Lecture 8æ ¸å¿ƒè¦ç‚¹ï¼š**
- è¯åµŒå…¥ï¼šä½ç»´ç¨ å¯†å‘é‡è¡¨ç¤ºè¯
- Word2Vecï¼šåŸºäºä¸Šä¸‹æ–‡è®­ç»ƒè¯å‘é‡
- GloVeï¼šåŸºäºå…¨å±€å…±ç°çŸ©é˜µ
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼šè®¡ç®—å‘é‡ç›¸ä¼¼æ€§
- è¯­ä¹‰è¿ç®—ï¼šking - man + woman â‰ˆ queen
- Poolingï¼šå°†è¯å‘é‡åˆå¹¶ä¸ºå¥å­å‘é‡ï¼ˆmean/max/minï¼‰
- åº”ç”¨ï¼šç”¨è¯åµŒå…¥åšæ–‡æœ¬åˆ†ç±»

**è®°ä½ï¼š**ç†è§£è¯åµŒå…¥çš„åŸç†ï¼ä¼šè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ä¼šåšpoolingï¼
