{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**COM3110/COM4115 Lab: Word Embeddings**\n","\n","This lab sheet was created by Jo√£o Augusto Leite"],"metadata":{"id":"vsEkwy-Ik7mJ"}},{"cell_type":"code","source":["!pip install nltk gensim scikit-learn plotly pandas numpy"],"metadata":{"id":"4fO6hUrM5sT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSQI68Mr8LAB"},"outputs":[],"source":["from nltk.corpus import gutenberg\n","from gensim.models import Word2Vec\n","from sklearn.manifold import TSNE\n","from matplotlib import pyplot\n","import gensim.downloader\n","import plotly.express as px\n","import pandas as pd\n","import nltk\n","import numpy as np\n","nltk.download('gutenberg')\n","nltk.download('punkt')"]},{"cell_type":"code","source":["# Helper function, you don't need to look into it.\n","def plot_vectors(X, annotations):\n","  X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=1).fit_transform(X)\n","  df = pd.DataFrame({'x': X_embedded[:, 0], 'y': X_embedded[:, 1], 'word': annotations})\n","  fig = px.scatter(df, x='x', y='y', text='word')\n","  fig.update_traces(textposition='top center')\n","  fig.show()"],"metadata":{"id":"pyfRRJlYFikX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 1: Computing word embeddings using Gensim"],"metadata":{"id":"0aLeseWSOq6Q"}},{"cell_type":"code","source":["# Loading the moby dick book corpus\n","sents = gutenberg.sents(\"melville-moby_dick.txt\")\n","sents = [[word.lower() for word in sent] for sent in sents]  # lowercasing"],"metadata":{"id":"mHfotQqv8Vrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"First sentence:\", sents[0])\n","print(\"First token in the first sentence:\", sents[0][0])"],"metadata":{"id":"KgLC7jU-POvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training the word2vec model is done in a single line.\n","# You can look into the Word2Vec class documentation for more details:\n","# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-parameters\n","\n","model = Word2Vec(sentences=sents, vector_size=100, window=5, min_count=1, workers=4).wv"],"metadata":{"id":"jAVRwiBC8ddv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# After trained, you can access the vectors like this.\n","# This object is a matrix where each row contains a 100-dimensional vector for each word in the corpus\n","model.vectors"],"metadata":{"id":"wdFyvYegGR2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.vectors.shape  # (n_words, 100) matrix"],"metadata":{"id":"8ww8QgZYGcB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mapping of words to indices in the matrix\n","print(model.key_to_index)"],"metadata":{"id":"uUZgVL8iHPI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.key_to_index[\"ship\"])"],"metadata":{"id":"57kF_LOdHmLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To retrieve the single vector representing a given word,\n","# you can first retrieve its index in the matrix, then access the matrix at that index:\n","\n","index_to_ship = model.key_to_index[\"ship\"]\n","model.vectors[index_to_ship]  # vector for the word 'ship'"],"metadata":{"id":"vUpAL-EzHtr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is another way to retrieve the vector of a word.\n","model['ship']"],"metadata":{"id":"GFs42CwaHhjR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all(model['ship'] == model.vectors[index_to_ship])  # all dimensions of both vectors are equal"],"metadata":{"id":"2y3NQ7jWIHmF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# It is possible that the model does not contain a particular word\n","# this will throw an error:\n","model['brontosaurus']"],"metadata":{"id":"PWKb5XAHenvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can check if there is a vector for a particular word in the matrix\n","# by checking if the word is in the model:\n","\"brontosaurus\" in model"],"metadata":{"id":"--NvW4Due6Wz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vector operations\n","You can perform mathematical operations using vectors. Note that a vector sum operation is an element-wise sum over each dimension of the vectors. This is done automatically by numpy when you use the + operator for two multidimensional vectors."],"metadata":{"id":"wzVXeYRSTXXo"}},{"cell_type":"code","source":["# This is vector sum operation between the vectors representing the words whale and ship.\n","u =  model['whale']\n","v = model['ship']\n","\n","print(u + v)"],"metadata":{"id":"AmJs7MeeK4pz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = u * v # element-wise multiplication\n","b = u / v # element-wise division\n","c = u + v # element-wise sum\n","d = u - v # element-wise difference\n","e = u @ v # dot product\n","\n","print(e)  # look at different results (a through e)"],"metadata":{"id":"OA7UW-OwTcZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Since you can perform mathematical operations using vectors, you are able\n","# to compute the cosine similarity between two vectors\n","\n","def cosine_similarity(u, v):\n","    dot_product = sum(a * b for a, b in zip(u, v))\n","    norm_u = np.sqrt(sum(a**2 for a in u))\n","    norm_v = np.sqrt(sum(b**2 for b in v))\n","\n","    if norm_u * norm_v != 0:  # avoid division by 0\n","      similarity = dot_product / (norm_u * norm_v)\n","    else:\n","      similarity = 0\n","\n","    return similarity\n","\n","u =  model['whale']\n","v = model['ship']\n","z = model['giraffe']\n","\n","print(\"Similarity between 'whale' and 'ship':\", cosine_similarity(u, v))\n","print(\"Similarity between 'whale' and 'giraffe':\", cosine_similarity(u, z))\n","print(\"Similarity between 'ship' and 'giraffe':\", cosine_similarity(u, z))"],"metadata":{"id":"lDNghrnVK5_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the 'plot_vectors' helper function to plot the vectors for 'whale', 'ship', and 'giraffe'.\n","\n","X = np.stack([u,v,z], axis=0)  # stack the three vectors vertically\n","print(X.shape)"],"metadata":{"id":"GcmAmQnXLigH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_vectors(X=X, annotations=[\"whale\", \"ship\", \"giraffe\"])"],"metadata":{"id":"Fu9JAn_uUiQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gensim also comes with a list of pre-trained models (i.e. you do not need to train a model yourself)\n","# You can also load models using gensim.downloader\n","\n","print(list(gensim.downloader.info()['models'].keys()))"],"metadata":{"id":"tho6UOJoVB94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can load one of these pre-trained models and use it off the shelf\n","# For instance, below you will be loading a word2vec model trained with similar data from Mikolov et al. (2013)\n","# Loading it will take a while, since the model is big!\n","\n","w2v_vectors = gensim.downloader.load('word2vec-google-news-300')"],"metadata":{"id":"uR7jzCfDY-GP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Since this is a more general and much bigger model, you should be able to replicate some of the semantic operations discussed in the lecture\n","# Write code to perform the 'king' - 'man' + 'woman' operation and compare the result with the vector of 'queen'\n","\n","# YOUR CODE HERE\n","\n","###"],"metadata":{"id":"MRyemKmTZ1nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the 'plot_vectors' helper function to plot the vectors for 'king', 'queen', 'man', 'woman' and 'king - man + woman'.\n","\n","# YOUR CODE HERE\n","\n","###"],"metadata":{"id":"1JuXkmyVj34A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 1: Write your own \"most_similar\" method.\n","Write your own function to retrieve the topn most similar words to a given word.\n","Return a list of tuples containing (word, similarity) in the same format as the model.most_similar method.\n","\n","**Note**: remember that the most similar word is always itself, and you don't need to include it."],"metadata":{"id":"WUk0nalhx12G"}},{"cell_type":"code","source":["# Write your own function to retrieve the topn most similar words to a given word.\n","# Return a list of tuples containing (word, similarity) in the same format\n","# as the model.most_similar method.\n","# Note: remember that the most similar word is always itself, and you don't need to include it.\n","\n","def get_top_similar_words(model, word, topn):\n","  # YOUR CODE HERE\n","\n","  ###\n","\n","  return top_similar_words  # list of tuples with (word, similarity)\n","\n","\n","top_words = get_top_similar_words(model, \"ship\", 50)\n","top_words"],"metadata":{"id":"cb-WZxmpGH70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use gensim method to calculate the most similar words\n","\n","topn = 50\n","sims = model.most_similar('ship', topn=topn) # get the most similar words\n","sims\n","\n","# This method returns the top n most similar words to the given word, along\n","# with their cosine similarities."],"metadata":{"id":"Qbx1PclzF76C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Is your implementation retrieving the exact same words as gensim's implementation?\n","all([t1[0] == t2[0] for t1, t2 in zip(top_words, sims)])"],"metadata":{"id":"Yz4efPQeJ_Kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the top n similar words.\n","# Note that this plot is a 2D representation of the 100-dimensional vectors,\n","# thus it is possible that you see slight differences from what your similarity\n","# scores indicated.\n","\n","your_top_words = [w for w, _ in top_words]\n","vectors = np.stack([model[w] for w in your_top_words])\n","plot_vectors(vectors, your_top_words)"],"metadata":{"id":"uDdQRNF0MdXx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 2: Text classification with embeddings\n","\n","In this section we will use embeddings that were precomputed by researchers at Stanford. The GloVe embeddings we will use were trained on a Twitter corpus with a total of 27 billion tokens and a vocabulary size of more than 1 million unique tokens.\n","You can read more about it at https://nlp.stanford.edu/projects/glove/."],"metadata":{"id":"X_ZgHTnYWvjS"}},{"cell_type":"code","source":["# Download and unzip the pretrained embeddings if you don't already have it\n","# Note: this can take a few minutes\n","!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n","!unzip glove.twitter.27B.zip"],"metadata":{"id":"CuxOWy-07eM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.scripts.glove2word2vec import glove2word2vec\n","from gensim.models import KeyedVectors\n","import pandas as pd"],"metadata":{"id":"nHb0FW1lXcPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the embeddings as a gensim model (this can take a few seconds)\n","glove_input_file = 'glove.twitter.27B.100d.txt'\n","model = KeyedVectors.load_word2vec_format(glove_input_file, binary=False, no_header=True)"],"metadata":{"id":"7uErxVehV6M7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.vectors.shape"],"metadata":{"id":"zsR3fiw0XYCB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note how the top similar words are different from the embeddings we trained previously.\n","sims = model.most_similar(\"ship\", topn=5)\n","sims"],"metadata":{"id":"VlRqPZfuYydk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the most similar words to 'whale' in the glove vector space\n","words = [w for w, _ in sims]\n","vectors = np.stack([model[w] for w in words], axis=0)\n","\n","plot_vectors(vectors, words)"],"metadata":{"id":"T6ceaowBY-oY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sentiment Classification\n","We will train a machine learning model to predict the sentiment of tweets associated with airline companies. We will use the embeddings of the tweets as features.\n","\n","**Please upload the Tweets_short.csv file in this notebook.** You can click the folder option \"Files\" in the left part of the UI, and then \"Upload to session storage\", or simply drag and drop the file."],"metadata":{"id":"cXe5iSVQ7H2_"}},{"cell_type":"code","source":["import re\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score"],"metadata":{"id":"2154IVIRkG_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"Tweets_short.csv\")\n","df"],"metadata":{"id":"NT-6mC6TrZMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Distribution of classes\n","df[\"airline_sentiment\"].value_counts()"],"metadata":{"id":"IsxXA8VWtA54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 1:\n","Implement normalisation and tokenisation for the dataset. The Glove embeddings we are going to use were trained by mapping hashtags to the token \\<hashtag>, numbers to the token \\<number>, urls to \\<url>, and @users to \\<user>. Also,  all tokens should be lowercased.\n","\n","The full list of normalisations can be found at https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb. You don't need to implement all normalisation steps. You can play around and see how the performance of your model is affected by extra normalisation steps."],"metadata":{"id":"mp4k7t2DybeE"}},{"cell_type":"code","source":["def normalise_tweet(tweet):\n","  ### YOUR CODE HERE\n","\n","  ###\n","\n","  return tweet\n","\n","def tokenise_tweet(tweet):\n","  ### YOUR CODE HERE\n","\n","  return tweet\n"],"metadata":{"id":"BsZlZ9Agyjk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mapping = {\"neutral\": 0, \"negative\": -1, \"positive\": 1}\n","labels = df[\"airline_sentiment\"].apply(lambda x: mapping[x]).to_numpy()  # convert the sentiment labels to 0, -1, and 1.\n","text = (df[\"text\"].apply(lambda x: normalise_tweet(x))  # normalise\n","  .apply(lambda x: tokenise_tweet(x)).tolist())  # tokenise"],"metadata":{"id":"Y-1wGWMscRWu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 2:\n","Implement a function to compute the embedding of a SENTENCE. Up until now we were working with WORD embeddings. Now you have to produce a single vector to represent the whole sentence. There are many ways to aggregate the word vectors into a sentence vector. We will implement a simple averaging method across the embeddings of each word in the sentence.\n","\n","Your function should sum the embeddings of the words in the tweet, and divide this vector by the number of tokens in the tweet.\n","\n","Remember to check if a word is in the model before retrieving it. Assign a vector of zeros if the word is not in the model: np.zeros(100).\n"],"metadata":{"id":"SSylY59C0cAh"}},{"cell_type":"code","source":["def get_tweet_embedding(tweet_tokenised):\n","  ### YOUR CODE HERE\n","\n","  ###\n","\n","\n","  return tweet_embedding"],"metadata":{"id":"klxZpt8viFDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Stack the tweet embeddings into a matrix X\n","X = []\n","for tokenised_tweet in text:\n","  tweet_embedding = get_tweet_embedding(tokenised_tweet)\n","  X.append(tweet_embedding)\n","\n","X = np.stack(X)\n","X.shape"],"metadata":{"id":"7sGH-VnxeDsL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We will use the tweet embeddings to train a classifier to predict if a given tweet\n","# has a positive, neutral, or negative sentiment.\n","print(\"Sentence embeddings:\", X)\n","print(\"Labels:\", labels)"],"metadata":{"id":"hup1HEDGn9UJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into train and test sets\n","train_idxs, test_idxs = train_test_split(range(len(df)), train_size=0.7, random_state=42, stratify=labels)\n","\n","X_train = X[train_idxs]\n","X_test = X[test_idxs]\n","\n","y_train = labels[train_idxs]\n","y_test = labels[test_idxs]"],"metadata":{"id":"qX1bjS0xd8xd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the classifier\n","clf = SVC(random_state=42, class_weight=\"balanced\")\n","clf.fit(X_train, y_train);"],"metadata":{"id":"y5d3CEYYdj11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions on the test set\n","predictions = clf.predict(X_test)\n","predictions"],"metadata":{"id":"RuUKRYJ9o4SR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Score the classifier\n","accuracy = accuracy_score(predictions, y_test)\n","f1_macro = f1_score(predictions, y_test, average=\"macro\")\n","print(f\"Accuracy: {accuracy*100:.2f}/100\")\n","print(f\"F1-Macro: {f1_macro*100:.2f}/100\")"],"metadata":{"id":"dUO6w5xQgxxb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 3:\n","Perform error analysis. Analyse the tweets that were incorrectly classified by your model. Explore the types of mistakes your classifier made. How many negative tweets did it predict as positive? and neutral?\n","\n","Take a look at [confusion matrices](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and try to use sklearn's implementation in your analysis."],"metadata":{"id":"UgK9tfNA1KPe"}},{"cell_type":"code","source":["# Analyse the tweets that were incorrectly classified by your model\n","misclassified_idxs = np.array(test_idxs)[predictions != y_test]\n","\n","pred_df = df.iloc[test_idxs].reset_index()\n","pred_df[\"predicted_sentiment\"] = predictions\n","\n","reverse_mapping = {v: k for k, v in mapping.items()}\n","pred_df[\"predicted_sentiment\"] = pred_df[\"predicted_sentiment\"].apply(lambda x: reverse_mapping[x])\n","mistakes_df = pred_df[pred_df[\"index\"].isin(misclassified_idxs)]\n","mistakes_df"],"metadata":{"id":"uLu5G_QDvn14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["condition = mistakes_df[\"airline_sentiment\"] == \"negative\"\n","mistakes_df[condition]"],"metadata":{"id":"97EBIL8oBWLp"},"execution_count":null,"outputs":[]}]}