# Lecture 7 è¶…åŸºç¡€å¤ä¹ ç¬”è®° - å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

## é›¶ã€å…ˆç†è§£é—®é¢˜ï¼ˆä»€ä¹ˆæ˜¯å‘½åå®ä½“è¯†åˆ«ï¼Ÿï¼‰

### ä»€ä¹ˆæ˜¯å‘½åå®ä½“ï¼ˆNamed Entityï¼‰ï¼Ÿ

**å®šä¹‰ï¼š**æ–‡æœ¬ä¸­å…·æœ‰ç‰¹å®šæ„ä¹‰çš„åç§°

**å¸¸è§ç±»å‹ï¼š**

```
äººåï¼ˆPERSONï¼‰ï¼š
  - "Rowan Atkinson"
  - "Mike Pence"

åœ°åï¼ˆLOCATIONï¼‰ï¼š
  - "Dutch"ï¼ˆè·å…°è¯­çš„ï¼ŒæŒ‡è·å…°ï¼‰
  - "British"ï¼ˆè‹±å›½çš„ï¼‰

ç»„ç»‡åï¼ˆORGANIZATIONï¼‰ï¼š
  - "BBC"
  - "Google"

æ—¥æœŸ/æ—¶é—´ï¼ˆDATE/TIMEï¼‰ï¼š
  - "January 20, 2017"
  - "2013"

å…¶ä»–ï¼ˆMISCï¼‰ï¼š
  - è¯­è¨€å
  - è´§å¸
  - ç™¾åˆ†æ¯”
```

---

### ä»€ä¹ˆæ˜¯å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ï¼Ÿ

**ä»»åŠ¡ï¼š**åœ¨æ–‡æœ¬ä¸­è‡ªåŠ¨è¯†åˆ«å¹¶æ ‡æ³¨å‘½åå®ä½“

**ä¾‹å­ï¼š**

```
è¾“å…¥æ–‡æœ¬ï¼š
"British actor Rowan Atkinson, best known as 'Mr. Bean,' has died."

NERè¾“å‡ºï¼š
British â† NORPï¼ˆå›½ç±/å®—æ•™/æ”¿æ²»å›¢ä½“ï¼‰
Rowan Atkinson â† PERSONï¼ˆäººåï¼‰
Mr. Bean â† PERSONï¼ˆäººåï¼‰
```

---

### ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

**åº”ç”¨åœºæ™¯ï¼š**

1. **ä¿¡æ¯æå–**
   ```
   ä»æ–°é—»ä¸­æå–ï¼šè°åœ¨å“ªé‡Œåšäº†ä»€ä¹ˆ
   â†’ æ„å»ºçŸ¥è¯†å›¾è°±
   ```

2. **é—®ç­”ç³»ç»Ÿ**
   ```
   é—®é¢˜ï¼š"è°æ˜¯è‹±å›½é¦–ç›¸ï¼Ÿ"
   â†’ éœ€è¦è¯†åˆ«"è‹±å›½"ï¼ˆåœ°åï¼‰å’Œ"é¦–ç›¸"ï¼ˆèŒä½ï¼‰
   ```

3. **äº‹å®æ ¸æŸ¥**
   ```
   è¯†åˆ«å£°æ˜ä¸­æåˆ°çš„äººç‰©ã€åœ°ç‚¹
   â†’ éªŒè¯çœŸå®æ€§
   ```

4. **å†…å®¹æ¨è**
   ```
   è¯†åˆ«ç”¨æˆ·å…³æ³¨çš„äººç‰©ã€åœ°ç‚¹
   â†’ æ¨èç›¸å…³å†…å®¹
   ```

---

## ä¸€ã€ä¸¤ç§NERæ–¹æ³•

### æ–¹æ³•1ï¼šåŸºäºè§„åˆ™ï¼ˆGazetteerï¼‰

**æ ¸å¿ƒæ€æƒ³ï¼š**ä½¿ç”¨é¢„å®šä¹‰çš„å®ä½“åˆ—è¡¨è¿›è¡ŒåŒ¹é…

```
æœ‰ä¸€ä¸ªå›½å®¶åˆ—è¡¨ï¼š
countries = ["USA", "UK", "China", "Brazil", ...]

åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾è¿™äº›è¯
â†’ å¦‚æœæ‰¾åˆ°ï¼Œæ ‡æ³¨ä¸º"COUNTRY"
```

**ä½¿ç”¨å·¥å…·ï¼š**GATEï¼ˆGeneral Architecture for Text Engineeringï¼‰

---

### æ–¹æ³•2ï¼šåŸºäºæœºå™¨å­¦ä¹ ï¼ˆNeural Networkï¼‰

**æ ¸å¿ƒæ€æƒ³ï¼š**è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹è‡ªåŠ¨å­¦ä¹ å®ä½“ç‰¹å¾

```
è®­ç»ƒæ•°æ®ï¼š
"Barack Obama was born in Hawaii" â†’ Obama=PERSON, Hawaii=LOCATION

æ¨¡å‹å­¦ä¹ ï¼š
- äººåç‰¹å¾ï¼šé€šå¸¸æ˜¯å¤§å†™å¼€å¤´çš„è¿ç»­è¯
- åœ°åç‰¹å¾ï¼šé€šå¸¸è·Ÿåœ¨"in", "at"ç­‰ä»‹è¯å
```

**ä½¿ç”¨å·¥å…·ï¼š**spaCy

---

## äºŒã€Gazetteeræ–¹æ³•ï¼ˆGATEï¼‰

### 1. ä»€ä¹ˆæ˜¯Gazetteerï¼Ÿ

**å®šä¹‰ï¼š**åœ°åè¯å…¸ï¼Œå­˜å‚¨é¢„å®šä¹‰å®ä½“çš„åˆ—è¡¨

**é€šä¿—ç†è§£ï¼š**ä¸€æœ¬"èŠ±åå†Œ"

```
å°±åƒå­¦æ ¡çš„å­¦ç”Ÿåå•ï¼š
- ä¸€ç­ï¼šå¼ ä¸‰ã€æå››ã€ç‹äº”
- äºŒç­ï¼šèµµå…­ã€å­™ä¸ƒã€å‘¨å…«

Gazetteerå°±æ˜¯å„ç§å®ä½“çš„åå•ï¼š
- å›½å®¶ï¼šUSA, UK, China, ...
- è¯­è¨€ï¼šEnglish, Chinese, Spanish, ...
- å…¬å¸ï¼šGoogle, Microsoft, Apple, ...
```

---

### 2. Gazetteerçš„ç»“æ„

**åŸºç¡€ç»“æ„ï¼š**

```
å®ä½“åç§° + ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
```

**ä¾‹å­ï¼š**

```
# countries.csv
Country,Code,WikiURL
Brazil,BR,https://en.wikipedia.org/wiki/Brazil
China,CN,https://en.wikipedia.org/wiki/China
USA,US,https://en.wikipedia.org/wiki/United_States

# languages.csv
Language,WikiURL
English,https://en.wikipedia.org/wiki/English_language
Chinese,https://en.wikipedia.org/wiki/Chinese_language
```

---

### 3. åˆ›å»ºGazetteer

#### æ­¥éª¤1ï¼šå‡†å¤‡æ•°æ®

```python
import pandas as pd

# è¯»å–å›½å®¶åˆ—è¡¨
countries = pd.read_csv("countries.csv")

# æ•°æ®ç¤ºä¾‹
#   Country  Code
# 0 Brazil   BR
# 1 China    CN
# 2 USA      US
```

---

#### æ­¥éª¤2ï¼šæ„å»ºWikipedia URL

**è§„åˆ™ï¼š**

```
åŸºç¡€URLï¼šhttps://en.wikipedia.org/wiki/
å®Œæ•´URLï¼šåŸºç¡€URL + å®ä½“åç§°

ä¾‹å­ï¼š
Brazil â†’ https://en.wikipedia.org/wiki/Brazil

ç‰¹æ®Šæƒ…å†µï¼ˆå¤šä¸ªå•è¯ï¼‰ï¼š
"United States" â†’ https://en.wikipedia.org/wiki/United_States
ï¼ˆç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼‰
```

**ä»£ç å®ç°ï¼š**

```python
def create_wiki_url(entity_name):
    """
    åˆ›å»ºWikipedia URL

    å‚æ•°ï¼š
    entity_nameï¼šå®ä½“åç§°

    è¿”å›ï¼š
    Wikipedia URL
    """
    # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
    name = entity_name.replace(" ", "_")

    # æ„å»ºURL
    url = f"https://en.wikipedia.org/wiki/{name}"

    return url
```

**ä¾‹å­ï¼š**

```python
create_wiki_url("Brazil")
# 'https://en.wikipedia.org/wiki/Brazil'

create_wiki_url("United States")
# 'https://en.wikipedia.org/wiki/United_States'

create_wiki_url("Bosnia and Herzegovina")
# 'https://en.wikipedia.org/wiki/Bosnia_and_Herzegovina'
```

---

#### æ­¥éª¤3ï¼šä½¿ç”¨GATEåˆ›å»ºGazetteer

**GATEçš„Gazetteeræ ¼å¼ï¼š**

```python
from gatenlp import Document
from gatenlp.processing.gazetteer import StringGazetteer

# åˆ›å»ºGazetteer
gaz = StringGazetteer()

# æ·»åŠ å®ä½“
gaz.add("Brazil", {"code": "BR", "type": "COUNTRY"})
gaz.add("China", {"code": "CN", "type": "COUNTRY"})
gaz.add("USA", {"code": "US", "type": "COUNTRY"})

# æˆ–è€…ä»åˆ—è¡¨æ‰¹é‡æ·»åŠ 
for _, row in countries.iterrows():
    gaz.add(row['Country'], {
        "code": row['Code'],
        "type": "COUNTRY",
        "wiki_url": create_wiki_url(row['Country'])
    })
```

---

#### æ­¥éª¤4ï¼šåº”ç”¨Gazetteeræ ‡æ³¨æ–‡æœ¬

```python
# åˆ›å»ºæ–‡æ¡£
text = "British actor Rowan Atkinson has died."
doc = Document(text)

# åº”ç”¨Gazetteer
gaz.run(doc)

# æŸ¥çœ‹æ ‡æ³¨ç»“æœ
for ann in doc.annset():
    print(f"å®ä½“: {doc[ann.start:ann.end]}")
    print(f"ç±»å‹: {ann.type}")
    print(f"ç‰¹å¾: {ann.features}")
    print()
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
å®ä½“: British
ç±»å‹: COUNTRY
ç‰¹å¾: {'code': 'GB', 'wiki_url': 'https://en.wikipedia.org/wiki/United_Kingdom'}
```

---

### 4. Gazetteerçš„ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹ï¼š**

```
1. ç®€å•ç›´æ¥
   â†’ åªéœ€è¦å‡†å¤‡åˆ—è¡¨

2. å‡†ç¡®ç‡é«˜ï¼ˆå¯¹äºåˆ—è¡¨ä¸­çš„å®ä½“ï¼‰
   â†’ å®Œå…¨åŒ¹é…ï¼Œä¸ä¼šå‡ºé”™

3. å¯æ§æ€§å¼º
   â†’ å¯ä»¥éšæ—¶æ·»åŠ /åˆ é™¤å®ä½“

4. æ— éœ€è®­ç»ƒ
   â†’ ä¸éœ€è¦æ ‡æ³¨æ•°æ®
```

**ç¼ºç‚¹ï¼š**

```
1. è¦†ç›–ä¸å…¨
   â†’ åªèƒ½è¯†åˆ«åˆ—è¡¨ä¸­çš„å®ä½“
   â†’ æ–°å®ä½“æ— æ³•è¯†åˆ«

2. æ­§ä¹‰é—®é¢˜
   ä¾‹å­ï¼š
   "Washington"
   â†’ å¯èƒ½æ˜¯äººåï¼ˆä¹”æ²»Â·åç››é¡¿ï¼‰
   â†’ å¯èƒ½æ˜¯åœ°åï¼ˆåç››é¡¿å·ã€åç››é¡¿ç‰¹åŒºï¼‰
   â†’ Gazetteeræ— æ³•åŒºåˆ†

3. ä¸Šä¸‹æ–‡é—®é¢˜
   ä¾‹å­ï¼š
   "I like turkey"
   â†’ "turkey"å¯èƒ½æ˜¯å›½å®¶åœŸè€³å…¶
   â†’ ä¹Ÿå¯èƒ½æ˜¯ç«é¸¡è‚‰
   â†’ Gazetteeråªçœ‹è¯ï¼Œä¸çœ‹ä¸Šä¸‹æ–‡

4. ç»´æŠ¤æˆæœ¬
   â†’ éœ€è¦ä¸æ–­æ›´æ–°åˆ—è¡¨
   â†’ æ–°çš„å®ä½“ï¼ˆæ–°å…¬å¸ã€æ–°äº§å“ï¼‰éœ€è¦æ‰‹åŠ¨æ·»åŠ 
```

---

## ä¸‰ã€æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆspaCyï¼‰

### 1. spaCyæ˜¯ä»€ä¹ˆï¼Ÿ

**å®šä¹‰ï¼š**ä¸€ä¸ªç°ä»£çš„NLPåº“ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæ–‡æœ¬å¤„ç†

**æ ¸å¿ƒç»„ä»¶ï¼š**

```
Processing Pipelineï¼ˆå¤„ç†æµæ°´çº¿ï¼‰ï¼š

æ–‡æœ¬ â†’ åˆ†è¯å™¨ â†’ è¯æ€§æ ‡æ³¨å™¨ â†’ ä¾å­˜å¥æ³•åˆ†æå™¨ â†’ NER â†’ è¾“å‡º
       (Tokenizer)  (POS Tagger)   (Dependency Parser)  (Named Entity Recognition)
```

**NERæ¨¡å‹ï¼š**

```
ä½¿ç”¨ç¥ç»ç½‘ç»œï¼ˆæ·±åº¦å­¦ä¹ ï¼‰
â†’ è‡ªåŠ¨å­¦ä¹ å®ä½“çš„ç‰¹å¾
â†’ ä¸éœ€è¦äººå·¥è§„åˆ™
```

---

### 2. spaCyçš„åŸºæœ¬ä½¿ç”¨

#### æ­¥éª¤1ï¼šå®‰è£…å’ŒåŠ è½½æ¨¡å‹

```python
import spacy

# åŠ è½½è‹±è¯­æ¨¡å‹
nlp = spacy.load("en_core_web_sm")
# en_core_web_smï¼šè‹±è¯­å°å‹æ¨¡å‹
# å…¶ä»–é€‰é¡¹ï¼š
# - en_core_web_mdï¼šä¸­å‹æ¨¡å‹ï¼ˆæ›´å‡†ç¡®ï¼‰
# - en_core_web_lgï¼šå¤§å‹æ¨¡å‹ï¼ˆæœ€å‡†ç¡®ï¼‰
```

---

#### æ­¥éª¤2ï¼šå¤„ç†æ–‡æœ¬

```python
text = "British actor Rowan Atkinson, best known as 'Mr. Bean,' has died."

# å¤„ç†æ–‡æœ¬
doc = nlp(text)

# docæ˜¯ä¸€ä¸ªDocumentå¯¹è±¡ï¼ŒåŒ…å«äº†æ‰€æœ‰çš„æ ‡æ³¨ä¿¡æ¯
```

---

#### æ­¥éª¤3ï¼šæå–å®ä½“

```python
# éå†æ‰€æœ‰å®ä½“
for ent in doc.ents:
    print(f"å®ä½“: {ent.text}")
    print(f"ç±»å‹: {ent.label_}")
    print(f"ä½ç½®: {ent.start_char} - {ent.end_char}")
    print()
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
å®ä½“: British
ç±»å‹: NORP
ä½ç½®: 0 - 7

å®ä½“: Rowan Atkinson
ç±»å‹: PERSON
ä½ç½®: 14 - 28

å®ä½“: Mr. Bean
ç±»å‹: PERSON
ä½ç½®: 48 - 56
```

---

### 3. spaCyçš„å®ä½“ç±»å‹

**å¸¸è§ç±»å‹ï¼š**

| æ ‡ç­¾ | è‹±æ–‡å…¨ç§° | ä¸­æ–‡ | ä¾‹å­ |
|------|---------|------|------|
| **PERSON** | Person | äººå | Rowan Atkinson, Mike Pence |
| **ORG** | Organization | ç»„ç»‡/å…¬å¸ | BBC, Google, United Nations |
| **GPE** | Geopolitical Entity | åœ°ç†æ”¿æ²»å®ä½“ | London, USA, China |
| **LOC** | Location | åœ°ç‚¹ | Pacific Ocean, Mount Everest |
| **DATE** | Date | æ—¥æœŸ | January 20, 2017, yesterday |
| **TIME** | Time | æ—¶é—´ | 3 PM, morning |
| **MONEY** | Money | è´§å¸ | $100, Â£50 |
| **PERCENT** | Percentage | ç™¾åˆ†æ¯” | 50%, 3.5% |
| **NORP** | Nationalities/Religious/Political groups | å›½ç±/å®—æ•™/æ”¿æ²»å›¢ä½“ | British, American, Muslim |
| **FAC** | Facility | è®¾æ–½ | Brooklyn Bridge, Heathrow Airport |
| **PRODUCT** | Product | äº§å“ | iPhone, Windows 10 |
| **EVENT** | Event | äº‹ä»¶ | World War II, Olympics |
| **WORK_OF_ART** | Work of Art | è‰ºæœ¯ä½œå“ | Hamlet, Mona Lisa |
| **LAW** | Law | æ³•å¾‹ | Constitution, Bill of Rights |
| **LANGUAGE** | Language | è¯­è¨€ | English, Chinese |

---

### 4. å®Œæ•´ç¤ºä¾‹

```python
import spacy

# åŠ è½½æ¨¡å‹
nlp = spacy.load("en_core_web_sm")

# å¤„ç†å¤šæ¡æ–‡æœ¬
texts = [
    "A video shows Dutch politician Tunahan Kuzu putting a grilled cheese in his pocket before an interview.",
    "British actor Rowan Atkinson, best known as 'Mr. Bean,' has died.",
    "Mike Pence once said that smoking doesn't kill people."
]

# åˆ†ææ¯æ¡æ–‡æœ¬
for text in texts:
    doc = nlp(text)

    print(f"æ–‡æœ¬: {text}")
    print(f"å®ä½“:")

    for ent in doc.ents:
        print(f"  - {ent.text} ({ent.label_})")

    print()
```

**è¾“å‡ºï¼š**

```
æ–‡æœ¬: A video shows Dutch politician Tunahan Kuzu putting a grilled cheese in his pocket before an interview.
å®ä½“:
  - Dutch (NORP)
  - Tunahan Kuzu (PERSON)

æ–‡æœ¬: British actor Rowan Atkinson, best known as 'Mr. Bean,' has died.
å®ä½“:
  - British (NORP)
  - Rowan Atkinson (PERSON)
  - Mr. Bean (PERSON)

æ–‡æœ¬: Mike Pence once said that smoking doesn't kill people.
å®ä½“:
  - Mike Pence (PERSON)
```

---

### 5. è‡ªå®šä¹‰Pipelineï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰

**é—®é¢˜ï¼š**é»˜è®¤pipelineåŒ…å«å¾ˆå¤šç»„ä»¶ï¼Œä½†æˆ‘ä»¬åªéœ€è¦NER

```python
# é»˜è®¤pipeline
nlp = spacy.load("en_core_web_sm")
# åŒ…å«ï¼štok2vec, tagger, parser, ner, attribute_ruler, lemmatizer

# è¿™äº›ç»„ä»¶ä¼šæ¶ˆè€—æ—¶é—´å’Œå†…å­˜
```

**è§£å†³æ–¹æ¡ˆï¼š**åªåŠ è½½éœ€è¦çš„ç»„ä»¶

```python
# åªåŠ è½½tokenizerå’Œner
nlp = spacy.load("en_core_web_sm",
                 disable=["tagger", "parser", "attribute_ruler", "lemmatizer"])

# æˆ–è€…æ˜ç¡®æŒ‡å®šè¦åŠ è½½çš„ç»„ä»¶
nlp = spacy.load("en_core_web_sm",
                 disable=[pipe for pipe in nlp.pipe_names if pipe not in ["tok2vec", "ner"]])
```

**æ•ˆæœï¼š**

```
åŠ è½½é€Ÿåº¦ï¼šå¿« 3-5 å€
å¤„ç†é€Ÿåº¦ï¼šå¿« 2-3 å€
å†…å­˜å ç”¨ï¼šå‡å°‘ 50%
```

---

## å››ã€å®ä½“ç»Ÿè®¡ä¸å¯è§†åŒ–

### 1. ç»Ÿè®¡æ¯æ¡æ–‡æœ¬çš„å®ä½“æ•°é‡

```python
import pandas as pd
import spacy

# åŠ è½½æ•°æ®
df = pd.read_csv("snopes.csv")

# åŠ è½½spaCyæ¨¡å‹
nlp = spacy.load("en_core_web_sm", disable=["tagger", "parser"])

# ç»Ÿè®¡å®ä½“æ•°é‡
entity_counts = []

for text in df['claim']:
    doc = nlp(text)
    entity_counts.append(len(doc.ents))

# æ·»åŠ åˆ°DataFrame
df['entity_count'] = entity_counts

# æŸ¥çœ‹ç»Ÿè®¡
print(df['entity_count'].describe())
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
count    500.0
mean       2.3
std        1.5
min        0.0
25%        1.0
50%        2.0
75%        3.0
max        8.0
```

**è§£é‡Šï¼š**
- å¹³å‡æ¯æ¡claimæœ‰2.3ä¸ªå®ä½“
- æœ€å°‘0ä¸ªï¼Œæœ€å¤š8ä¸ª
- 50%çš„claimæœ‰2ä¸ªæˆ–æ›´å°‘å®ä½“

---

### 2. ç»˜åˆ¶åˆ†å¸ƒå›¾

```python
import matplotlib.pyplot as plt

# ç»˜åˆ¶ç›´æ–¹å›¾
plt.figure(figsize=(10, 6))
plt.hist(df['entity_count'], bins=range(0, df['entity_count'].max()+2),
         edgecolor='black', alpha=0.7)
plt.xlabel('Number of Entities per Claim')
plt.ylabel('Frequency')
plt.title('Distribution of Entity Counts')
plt.grid(axis='y', alpha=0.3)
plt.show()
```

**å›¾å½¢ç¤ºä¾‹ï¼š**

```
     é¢‘ç‡
     â†‘
120  |    â–ˆ
100  |    â–ˆ
 80  |    â–ˆ  â–ˆ
 60  |    â–ˆ  â–ˆ
 40  |    â–ˆ  â–ˆ  â–ˆ
 20  |    â–ˆ  â–ˆ  â–ˆ  â–ˆ
  0  |____â–ˆ__â–ˆ__â–ˆ__â–ˆ__â–ˆ__â–ˆ__â–ˆ__â–ˆ___
          0  1  2  3  4  5  6  7  8  â†’ å®ä½“æ•°é‡

å¤§éƒ¨åˆ†claimæœ‰1-3ä¸ªå®ä½“
```

---

### 3. ç»Ÿè®¡æœ€å¸¸è§çš„å®ä½“

```python
from collections import Counter

# æ”¶é›†æ‰€æœ‰å®ä½“
all_entities = []

for text in df['claim']:
    doc = nlp(text)
    for ent in doc.ents:
        all_entities.append(ent.text)

# ç»Ÿè®¡é¢‘ç‡
entity_counter = Counter(all_entities)

# æ‰¾å‡ºæœ€å¸¸è§çš„20ä¸ª
top_20 = entity_counter.most_common(20)

# æ‰“å°
for entity, count in top_20:
    print(f"{entity}: {count}")
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
United States: 45
Trump: 32
Obama: 28
Clinton: 25
2016: 20
New York: 18
Facebook: 15
Russia: 14
...
```

---

### 4. æŒ‰ç±»å‹ç»Ÿè®¡å®ä½“

```python
# æ”¶é›†å®ä½“åŠå…¶ç±»å‹
entity_type_counter = Counter()

for text in df['claim']:
    doc = nlp(text)
    for ent in doc.ents:
        entity_type_counter[ent.label_] += 1

# æ‰“å°ç»Ÿè®¡
for ent_type, count in entity_type_counter.most_common():
    print(f"{ent_type}: {count}")
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
PERSON: 250
GPE: 180
DATE: 120
ORG: 95
NORP: 60
MONEY: 30
...
```

---

## äº”ã€å®ä½“å…±ç°åˆ†æ

### 1. ä»€ä¹ˆæ˜¯å®ä½“å…±ç°ï¼Ÿ

**å®šä¹‰ï¼š**ä¸¤ä¸ªå®ä½“åœ¨åŒä¸€æ¡æ–‡æœ¬ä¸­å‡ºç°

**ä¾‹å­ï¼š**

```
æ–‡æœ¬ï¼š"Donald Trump met Vladimir Putin in Moscow"

å®ä½“ï¼š
- Donald Trump (PERSON)
- Vladimir Putin (PERSON)
- Moscow (GPE)

å…±ç°å¯¹ï¼š
- (Donald Trump, Vladimir Putin)
- (Donald Trump, Moscow)
- (Vladimir Putin, Moscow)
```

**æ„ä¹‰ï¼š**

```
å…±ç°é¢‘ç‡é«˜ â†’ ä¸¤ä¸ªå®ä½“å…³ç³»å¯†åˆ‡

ä¾‹å¦‚ï¼š
"Obama" å’Œ "White House" ç»å¸¸å…±ç°
â†’ Obamaæ˜¯å‰æ€»ç»Ÿï¼Œåœ¨ç™½å®«å·¥ä½œè¿‡
```

---

### 2. æå–å…±ç°å…³ç³»

```python
from itertools import combinations
from collections import Counter

# æ”¶é›†æ‰€æœ‰å…±ç°å¯¹
cooccurrences = Counter()

for text in df['claim']:
    doc = nlp(text)

    # æå–æœ¬æ–‡ä¸­çš„æ‰€æœ‰å®ä½“
    entities = [ent.text for ent in doc.ents]

    # å¦‚æœå°‘äº2ä¸ªå®ä½“ï¼Œè·³è¿‡
    if len(entities) < 2:
        continue

    # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å®ä½“å¯¹
    for ent1, ent2 in combinations(entities, 2):
        # æŒ‰å­—æ¯é¡ºåºæ’åºï¼ˆç¡®ä¿(A,B)å’Œ(B,A)è¢«è®¤ä¸ºæ˜¯åŒä¸€å¯¹ï¼‰
        pair = tuple(sorted([ent1, ent2]))
        cooccurrences[pair] += 1

# æ‰“å°æœ€å¸¸è§çš„å…±ç°å¯¹
for (ent1, ent2), count in cooccurrences.most_common(20):
    print(f"{ent1} <-> {ent2}: {count}")
```

**è¯¦ç»†è§£é‡Šï¼š**

**`combinations` å‡½æ•°ï¼š**

```python
from itertools import combinations

entities = ['A', 'B', 'C']

# ç”Ÿæˆæ‰€æœ‰2ä¸ªå…ƒç´ çš„ç»„åˆ
for pair in combinations(entities, 2):
    print(pair)

# è¾“å‡ºï¼š
# ('A', 'B')
# ('A', 'C')
# ('B', 'C')

# æ³¨æ„ï¼šä¸åŒ…æ‹¬ ('A', 'A'), ('B', 'B'), ('C', 'C')
# ä¹Ÿä¸åŒ…æ‹¬é‡å¤çš„ï¼Œå¦‚ ('B', 'A')ï¼ˆå› ä¸ºå·²ç»æœ‰('A', 'B')äº†ï¼‰
```

**ä¸ºä»€ä¹ˆè¦æ’åºï¼Ÿ**

```python
# å‡è®¾æœ‰ä¸¤æ¡æ–‡æœ¬ï¼š
# æ–‡æœ¬1ï¼š"Obama met Trump"  â†’ (Obama, Trump)
# æ–‡æœ¬2ï¼š"Trump criticized Obama"  â†’ (Trump, Obama)

# ä¸æ’åºï¼š
cooccurrences[('Obama', 'Trump')] = 1
cooccurrences[('Trump', 'Obama')] = 1
# ä¸¤ä¸ªä¸åŒçš„é”®ï¼

# æ’åºåï¼š
pair1 = tuple(sorted(['Obama', 'Trump']))  # ('Obama', 'Trump')
pair2 = tuple(sorted(['Trump', 'Obama']))  # ('Obama', 'Trump')
cooccurrences[('Obama', 'Trump')] = 2
# åŒä¸€ä¸ªé”®ï¼
```

---

**è¾“å‡ºç¤ºä¾‹ï¼š**

```
United States <-> Donald Trump: 25
Barack Obama <-> White House: 18
Hillary Clinton <-> Donald Trump: 15
Russia <-> Vladimir Putin: 12
Facebook <-> United States: 10
...
```

---

### 3. å¯è§†åŒ–å…±ç°ç½‘ç»œ

**ç›®æ ‡ï¼š**åˆ›å»ºä¸€ä¸ªç½‘ç»œå›¾ï¼Œå±•ç¤ºå®ä½“ä¹‹é—´çš„å…³ç³»

**ä½¿ç”¨å·¥å…·ï¼š**NetworkX + Matplotlib

```python
import networkx as nx
import matplotlib.pyplot as plt

# åˆ›å»ºå›¾
G = nx.Graph()

# åªæ·»åŠ é¢‘ç‡ >= 5 çš„å…±ç°å¯¹
for (ent1, ent2), count in cooccurrences.items():
    if count >= 5:
        G.add_edge(ent1, ent2, weight=count)

# ç»˜åˆ¶ç½‘ç»œå›¾
plt.figure(figsize=(15, 15))

# ä½¿ç”¨spring layout
pos = nx.spring_layout(G, k=0.5, iterations=50)

# ç»˜åˆ¶èŠ‚ç‚¹
nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')

# ç»˜åˆ¶è¾¹ï¼ˆç²—ç»†æ ¹æ®æƒé‡ï¼‰
edges = G.edges()
weights = [G[u][v]['weight'] for u, v in edges]
nx.draw_networkx_edges(G, pos, width=[w*0.5 for w in weights], alpha=0.5)

# ç»˜åˆ¶æ ‡ç­¾
nx.draw_networkx_labels(G, pos, font_size=10)

plt.title("Entity Co-occurrence Network")
plt.axis('off')
plt.tight_layout()
plt.show()
```

**å›¾å½¢è§£é‡Šï¼š**

```
- èŠ‚ç‚¹ï¼šå®ä½“
- è¾¹ï¼šå…±ç°å…³ç³»
- è¾¹çš„ç²—ç»†ï¼šå…±ç°é¢‘ç‡ï¼ˆè¶Šç²—è¶Šé¢‘ç¹ï¼‰

ä¾‹å¦‚ï¼š
    Obama â€”â€”â€”â€”â€”â€”â€”â€”â€” White House  ï¼ˆç²—çº¿ï¼Œé¢‘ç¹å…±ç°ï¼‰
      |                 |
      |                 |
    Trump â€”â€”â€”â€”â€”â€”â€”â€” United States  ï¼ˆç²—çº¿ï¼‰
      |
      |
    Putin â€”â€”â€” Russia  ï¼ˆç»†çº¿ï¼Œè¾ƒå°‘å…±ç°ï¼‰
```

---

## å…­ã€å¤§å°å†™å¯¹NERçš„å½±å“

### é—®é¢˜

**å¤§å†™ï¼š**

```python
text = "BARACK OBAMA WAS BORN IN HAWAII"
doc = nlp(text)
for ent in doc.ents:
    print(ent.text, ent.label_)

# å¯èƒ½è¾“å‡ºï¼š
# BARACK OBAMA PERSON  âœ… æ­£ç¡®
# HAWAII GPE  âœ… æ­£ç¡®
```

**å°å†™ï¼š**

```python
text = "barack obama was born in hawaii"
doc = nlp(text)
for ent in doc.ents:
    print(ent.text, ent.label_)

# å¯èƒ½è¾“å‡ºï¼š
# ï¼ˆæ²¡æœ‰å®ä½“ï¼ï¼‰âŒ é”™è¯¯
```

---

### ä¸ºä»€ä¹ˆï¼Ÿ

**NERæ¨¡å‹ä¾èµ–å¤§å°å†™ç‰¹å¾ï¼š**

```
è‹±è¯­ä¸­ï¼Œä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åï¼‰é€šå¸¸é¦–å­—æ¯å¤§å†™
â†’ æ¨¡å‹å­¦ä¹ åˆ°äº†è¿™ä¸ªç‰¹å¾
â†’ å°å†™æ–‡æœ¬å¤±å»äº†è¿™ä¸ªé‡è¦çº¿ç´¢
```

**ä¾‹å­ï¼š**

```
"I saw a turkey in Turkey"

æ­£å¸¸å¤§å°å†™ï¼š
- turkey (å°å†™) â†’ å¯èƒ½æ˜¯åŠ¨ç‰©
- Turkey (å¤§å†™) â†’ å¯èƒ½æ˜¯å›½å®¶

å…¨å°å†™ï¼š
- turkey â†’ æ— æ³•åŒºåˆ†
```

---

### è§£å†³æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šä¿æŒåŸå§‹å¤§å°å†™**

```python
# ä¸è¦é¢„å¤„ç†æ—¶å°±è½¬å°å†™
# é”™è¯¯ï¼š
text = text.lower()  âŒ

# æ­£ç¡®ï¼š
text = text  # ä¿æŒåŸæ ·
```

---

**æ–¹æ¡ˆ2ï¼šä½¿ç”¨å¤§å°å†™ä¸æ•æ„Ÿçš„æ¨¡å‹**

```python
# spaCyçš„æŸäº›æ¨¡å‹å¯¹å¤§å°å†™ä¸é‚£ä¹ˆæ•æ„Ÿ
# ä½†å‡†ç¡®ç‡å¯èƒ½ä¸‹é™
```

---

**æ–¹æ¡ˆ3ï¼šæ¢å¤å¤§å°å†™ï¼ˆå¦‚æœå¿…é¡»è¦å°å†™ï¼‰**

```python
# å¦‚æœæ–‡æœ¬å·²ç»æ˜¯å°å†™ï¼Œå°è¯•æ¢å¤
# è¿™é€šå¸¸å¾ˆå›°éš¾ä¸”ä¸å‡†ç¡®
```

---

## ä¸ƒã€æ¨¡å‹çš„ç¼ºé™·ï¼ˆ"Fool the Model"ï¼‰

### 1. æ–°å®ä½“æ— æ³•è¯†åˆ«

**é—®é¢˜ï¼š**

```python
text = "Elon Musk founded SpaceX in 2002"
doc = nlp(text)

# å¦‚æœæ¨¡å‹è®­ç»ƒæ•°æ®æ˜¯2010å¹´ä¹‹å‰çš„
# å¯èƒ½æ— æ³•è¯†åˆ«"Elon Musk"å’Œ"SpaceX"ï¼ˆå› ä¸ºé‚£æ—¶å®ƒä»¬ä¸å‡ºåï¼‰
```

---

### 2. ä¸Šä¸‹æ–‡æ­§ä¹‰

**ä¾‹å­1ï¼š"Washington"**

```python
text1 = "George Washington was the first president"
# Washington â†’ PERSON âœ…

text2 = "I visited Washington last year"
# Washington â†’ GPE âœ…

text3 = "Washington approved the budget"
# Washington â†’ ??? (å¯èƒ½æ˜¯äººåï¼Œå¯èƒ½æ˜¯åœ°å)
# æ¨¡å‹å¯èƒ½ææ··
```

---

**ä¾‹å­2ï¼š"Turkey"**

```python
text1 = "I visited Turkey last summer"
# Turkey â†’ GPE âœ…

text2 = "I ate turkey for Thanksgiving"
# turkey â†’ ä¸æ˜¯å®ä½“ âœ…

text3 = "Turkey is delicious"
# Turkey â†’ GPE? è¿˜æ˜¯é£Ÿç‰©ï¼Ÿ
# æ¨¡å‹å¯èƒ½é”™è¯¯
```

---

### 3. ç¼©å†™å’Œç®€ç§°

```python
text = "The UK voted to leave the EU"
# UK â†’ GPE âœ…
# EU â†’ ORG âœ…

# ä½†æ˜¯ï¼š
text = "My friend lives in the uk"
# uk (å°å†™) â†’ å¯èƒ½æ— æ³•è¯†åˆ« âŒ
```

---

### 4. åµŒå¥—å®ä½“

```python
text = "University of California, Berkeley"
# è¿™æ˜¯ä¸€ä¸ªå®ä½“
# ä½†å¯èƒ½è¢«è¯†åˆ«ä¸ºï¼š
# - "University of California" (ORG)
# - "Berkeley" (GPE)
# ä¸¤ä¸ªç‹¬ç«‹å®ä½“ï¼Œè€Œä¸æ˜¯ä¸€ä¸ª

# æˆ–è€…å®Œå…¨è¯†åˆ«ä¸ºï¼š
# - "University of California, Berkeley" (ORG) âœ… æ­£ç¡®
```

---

### 5. åˆ›é€ æ€§/éæ­£å¼è¡¨è¾¾

```python
text = "The Donald tweeted again"
# "The Donald" â†’ æŒ‡Donald Trump
# ä½†æ¨¡å‹å¯èƒ½æ— æ³•è¯†åˆ«

text = "BeyoncÃ© dropped a new album"
# "BeyoncÃ©" â†’ å¯èƒ½å› ä¸ºç‰¹æ®Šå­—ç¬¦æ— æ³•è¯†åˆ«

text = "I met J.Lo at the airport"
# "J.Lo" â†’ Jennifer Lopezçš„æ˜µç§°
# æ¨¡å‹å¯èƒ½æ— æ³•è¯†åˆ«
```

---

## å…«ã€è¯„ä¼°NERæ¨¡å‹

### 1. è¯„ä¼°æŒ‡æ ‡

**ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼š**

```
ç²¾ç¡®ç‡ = æ­£ç¡®è¯†åˆ«çš„å®ä½“æ•° / æ‰€æœ‰è¯†åˆ«å‡ºçš„å®ä½“æ•°

ä¾‹å¦‚ï¼š
æ¨¡å‹è¯†åˆ«äº†100ä¸ªå®ä½“
å…¶ä¸­80ä¸ªæ˜¯æ­£ç¡®çš„
20ä¸ªæ˜¯é”™è¯¯çš„ï¼ˆè¯¯æŠ¥ï¼‰

ç²¾ç¡®ç‡ = 80 / 100 = 80%
```

---

**å¬å›ç‡ï¼ˆRecallï¼‰ï¼š**

```
å¬å›ç‡ = æ­£ç¡®è¯†åˆ«çš„å®ä½“æ•° / åº”è¯¥è¯†åˆ«çš„å®ä½“æ•°

ä¾‹å¦‚ï¼š
æ–‡æœ¬ä¸­å®é™…æœ‰120ä¸ªå®ä½“
æ¨¡å‹åªè¯†åˆ«å‡ºäº†80ä¸ª

å¬å›ç‡ = 80 / 120 = 66.7%
```

---

**F1åˆ†æ•°ï¼ˆF1 Scoreï¼‰ï¼š**

```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

è°ƒå’Œå¹³å‡æ•°ï¼Œç»¼åˆè€ƒè™‘ç²¾ç¡®ç‡å’Œå¬å›ç‡

ä¾‹å¦‚ï¼š
Precision = 80%
Recall = 66.7%

F1 = 2 Ã— (0.8 Ã— 0.667) / (0.8 + 0.667)
   = 2 Ã— 0.534 / 1.467
   = 0.728
   = 72.8%
```

---

### 2. æ··æ·†çŸ©é˜µç¤ºä¾‹

**å¯¹äºå¤šç±»åˆ«NERï¼š**

```
         é¢„æµ‹
å®é™…    PERSON  ORG   GPE  å…¶ä»–
PERSON [  80    5     2    13  ]
ORG    [  3     60    1    6   ]
GPE    [  1     2     70   7   ]
å…¶ä»–   [  10    8     5    177 ]

å¯¹è§’çº¿ï¼šæ­£ç¡®è¯†åˆ«
éå¯¹è§’çº¿ï¼šé”™è¯¯è¯†åˆ«

ä¾‹å¦‚ï¼š
- 80ä¸ªPERSONè¢«æ­£ç¡®è¯†åˆ«ä¸ºPERSON
- 5ä¸ªPERSONè¢«é”™è¯¯è¯†åˆ«ä¸ºORG
- 3ä¸ªORGè¢«é”™è¯¯è¯†åˆ«ä¸ºPERSON
```

---

## ä¹ã€å®é™…åº”ç”¨å»ºè®®

### 1. é€‰æ‹©åˆé€‚çš„æ–¹æ³•

**ä½¿ç”¨Gazetteerï¼Œå¦‚æœï¼š**

```
1. å®ä½“åˆ—è¡¨å›ºå®šä¸”å¯æšä¸¾
   ä¾‹å¦‚ï¼šå›½å®¶ã€å·ã€åŸå¸‚åˆ—è¡¨

2. å‡†ç¡®ç‡è¦æ±‚é«˜
   ä¾‹å¦‚ï¼šæ³•å¾‹æ–‡ä»¶ã€åŒ»ç–—è®°å½•

3. ç‰¹å®šé¢†åŸŸ
   ä¾‹å¦‚ï¼šå…¬å¸åç§°ã€äº§å“å‹å·
```

---

**ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆspaCyï¼‰ï¼Œå¦‚æœï¼š**

```
1. å®ä½“ç§ç±»å¤šæ ·
   ä¾‹å¦‚ï¼šæ–°é—»æ–‡ç« ã€ç¤¾äº¤åª’ä½“

2. éœ€è¦å¤„ç†æ–°å®ä½“
   ä¾‹å¦‚ï¼šè¯†åˆ«æ–°å…¬å¸ã€æ–°äººç‰©

3. éœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡
   ä¾‹å¦‚ï¼šåŒºåˆ†"Apple"ï¼ˆå…¬å¸ï¼‰å’Œ"apple"ï¼ˆæ°´æœï¼‰
```

---

### 2. ç»„åˆä¸¤ç§æ–¹æ³•

**æœ€ä½³å®è·µï¼š**

```
æ­¥éª¤1ï¼šç”¨spaCyè¯†åˆ«å¤§éƒ¨åˆ†å®ä½“
æ­¥éª¤2ï¼šç”¨Gazetteerè¡¥å……ç‰¹å®šé¢†åŸŸçš„å®ä½“
æ­¥éª¤3ï¼šåˆå¹¶ç»“æœ

ä¾‹å¦‚ï¼š
spaCyè¯†åˆ«ï¼šäººåã€æ—¥æœŸã€åœ°åç­‰é€šç”¨å®ä½“
Gazetteerè¡¥å……ï¼šå…¬å¸å†…éƒ¨çš„äº§å“åç§°ã€éƒ¨é—¨åç§°ç­‰
```

---

### 3. é¢„å¤„ç†å»ºè®®

**ä¿æŒå¤§å°å†™ï¼š**

```python
# ä¸è¦ï¼š
text = text.lower()  âŒ

# è¦ï¼š
text = text.strip()  âœ… åªå»é™¤é¦–å°¾ç©ºæ ¼
```

---

**å»é™¤é‡å¤ï¼š**

```python
import pandas as pd

df = pd.read_csv("data.csv")
df = df.drop_duplicates()  # å»é™¤é‡å¤è¡Œ
```

---

**å¤„ç†ç¼ºå¤±å€¼ï¼š**

```python
# å»é™¤ç©ºç™½æ–‡æœ¬
df = df[df['text'].notna()]
df = df[df['text'].str.strip() != '']
```

---

## åã€çº¸ç¬”è€ƒè¯•é‡ç‚¹

### å¿…é¡»æŒæ¡

#### 1. âœ… **NERçš„å®šä¹‰å’Œç›®çš„**

```
å®šä¹‰ï¼šè¯†åˆ«æ–‡æœ¬ä¸­çš„å‘½åå®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡åç­‰ï¼‰
ç›®çš„ï¼šä¿¡æ¯æå–ã€çŸ¥è¯†å›¾è°±æ„å»ºã€é—®ç­”ç³»ç»Ÿç­‰
```

---

#### 2. âœ… **ä¸¤ç§NERæ–¹æ³•çš„å¯¹æ¯”**

```
Gazetteerï¼ˆåŸºäºè§„åˆ™ï¼‰ï¼š
  ä¼˜ç‚¹ï¼šå‡†ç¡®ç‡é«˜ã€å¯æ§
  ç¼ºç‚¹ï¼šè¦†ç›–ä¸å…¨ã€ç»´æŠ¤æˆæœ¬é«˜

æœºå™¨å­¦ä¹ ï¼ˆç¥ç»ç½‘ç»œï¼‰ï¼š
  ä¼˜ç‚¹ï¼šæ³›åŒ–èƒ½åŠ›å¼ºã€å¯å¤„ç†æ–°å®ä½“
  ç¼ºç‚¹ï¼šå¯èƒ½å‡ºé”™ã€éœ€è¦è®­ç»ƒæ•°æ®
```

---

#### 3. âœ… **å¸¸è§å®ä½“ç±»å‹**

```
PERSONï¼šäººå
GPEï¼šåœ°ç†æ”¿æ²»å®ä½“ï¼ˆå›½å®¶ã€åŸå¸‚ç­‰ï¼‰
ORGï¼šç»„ç»‡
DATEï¼šæ—¥æœŸ
NORPï¼šå›½ç±/å®—æ•™/æ”¿æ²»å›¢ä½“
```

---

#### 4. âœ… **å®ä½“å…±ç°çš„æ¦‚å¿µ**

```
å®šä¹‰ï¼šä¸¤ä¸ªå®ä½“åœ¨åŒä¸€æ–‡æœ¬ä¸­å‡ºç°
æ„ä¹‰ï¼šå‘ç°å®ä½“ä¹‹é—´çš„å…³ç³»
```

---

#### 5. âœ… **å¤§å°å†™å¯¹NERçš„å½±å“**

```
å¤§å†™ï¼šæä¾›é‡è¦çš„ç‰¹å¾ä¿¡æ¯
å°å†™ï¼šå¯èƒ½å¯¼è‡´å®ä½“æ— æ³•è¯†åˆ«
```

---

### å¯èƒ½çš„è€ƒé¢˜

#### é¢˜å‹1ï¼šè¯†åˆ«å®ä½“

**é¢˜ç›®ï¼š**æ ‡æ³¨ä»¥ä¸‹æ–‡æœ¬ä¸­çš„å‘½åå®ä½“

```
"Mike Pence once said that smoking doesn't kill people."
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
Mike Pence â† PERSONï¼ˆäººåï¼‰

è§£é‡Šï¼š
- "Mike Pence" æ˜¯ç¾å›½å‰å‰¯æ€»ç»Ÿçš„åå­—
- smokingã€people ä¸æ˜¯å‘½åå®ä½“ï¼ˆåªæ˜¯æ™®é€šè¯æ±‡ï¼‰
```
</details>

---

#### é¢˜å‹2ï¼šå®ä½“ç±»å‹åˆ¤æ–­

**é¢˜ç›®ï¼š**åˆ¤æ–­ä»¥ä¸‹å®ä½“çš„ç±»å‹

```
1. "United Nations"
2. "January 20, 2017"
3. "British"
4. "$100"
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
1. "United Nations" â†’ ORGï¼ˆç»„ç»‡ï¼‰
   - è”åˆå›½æ˜¯å›½é™…ç»„ç»‡

2. "January 20, 2017" â†’ DATEï¼ˆæ—¥æœŸï¼‰
   - æ˜ç¡®çš„æ—¥æœŸ

3. "British" â†’ NORPï¼ˆå›½ç±/å®—æ•™/æ”¿æ²»å›¢ä½“ï¼‰
   - æŒ‡è‹±å›½äººã€è‹±å›½çš„

4. "$100" â†’ MONEYï¼ˆè´§å¸ï¼‰
   - ç¾å…ƒé‡‘é¢
```
</details>

---

#### é¢˜å‹3ï¼šæ–¹æ³•é€‰æ‹©

**é¢˜ç›®ï¼š**ä»¥ä¸‹åœºæ™¯åº”è¯¥ä½¿ç”¨Gazetteerè¿˜æ˜¯æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Ÿ

```
åœºæ™¯1ï¼šè¯†åˆ«æ‰€æœ‰ç¾å›½50ä¸ªå·çš„åç§°
åœºæ™¯2ï¼šè¯†åˆ«æ–°é—»æ–‡ç« ä¸­çš„æ‰€æœ‰äººå
åœºæ™¯3ï¼šè¯†åˆ«å…¬å¸å†…éƒ¨çš„äº§å“ä»£å·
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
åœºæ™¯1ï¼šGazetteer
  ç†ç”±ï¼š
  - ç¾å›½å·çš„åˆ—è¡¨æ˜¯å›ºå®šçš„ï¼ˆ50ä¸ªï¼‰
  - ä¸ä¼šæœ‰æ–°çš„å·å‡ºç°
  - å‡†ç¡®ç‡è¦æ±‚é«˜

åœºæ™¯2ï¼šæœºå™¨å­¦ä¹ ï¼ˆspaCyï¼‰
  ç†ç”±ï¼š
  - äººåç§ç±»ç¹å¤šï¼Œæ— æ³•æšä¸¾
  - æ–°äººç‰©ä¸æ–­å‡ºç°
  - éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­

åœºæ™¯3ï¼šGazetteer
  ç†ç”±ï¼š
  - å…¬å¸äº§å“ä»£å·æ˜¯å†…éƒ¨å®šä¹‰çš„
  - æœ‰æ˜ç¡®çš„åˆ—è¡¨
  - å¤–éƒ¨æ¨¡å‹ä¸çŸ¥é“è¿™äº›ç‰¹å®šåç§°
```
</details>

---

#### é¢˜å‹4ï¼šå…±ç°åˆ†æ

**é¢˜ç›®ï¼š**ç»™å®šä»¥ä¸‹3æ¡æ–‡æœ¬ï¼Œæ‰¾å‡ºæ‰€æœ‰å®ä½“å…±ç°å¯¹

```
æ–‡æœ¬1ï¼š"Obama met Putin in Moscow"
æ–‡æœ¬2ï¼š"Trump criticized Obama"
æ–‡æœ¬3ï¼š"Putin visited Moscow"
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
é¦–å…ˆè¯†åˆ«å®ä½“ï¼š
æ–‡æœ¬1ï¼šObama (PERSON), Putin (PERSON), Moscow (GPE)
æ–‡æœ¬2ï¼šTrump (PERSON), Obama (PERSON)
æ–‡æœ¬3ï¼šPutin (PERSON), Moscow (GPE)

æ–‡æœ¬1çš„å…±ç°å¯¹ï¼š
- (Obama, Putin)
- (Obama, Moscow)
- (Putin, Moscow)

æ–‡æœ¬2çš„å…±ç°å¯¹ï¼š
- (Obama, Trump)

æ–‡æœ¬3çš„å…±ç°å¯¹ï¼š
- (Putin, Moscow)

ç»Ÿè®¡é¢‘ç‡ï¼š
- (Putin, Moscow): 2æ¬¡
- (Obama, Putin): 1æ¬¡
- (Obama, Moscow): 1æ¬¡
- (Obama, Trump): 1æ¬¡

æœ€å¸¸è§çš„å…±ç°å¯¹ï¼š(Putin, Moscow)
```
</details>

---

#### é¢˜å‹5ï¼šæ¨¡å‹ç¼ºé™·åˆ†æ

**é¢˜ç›®ï¼š**ä¸ºä»€ä¹ˆä»¥ä¸‹æ–‡æœ¬å¯èƒ½ä¼š"æ¬ºéª—"NERæ¨¡å‹ï¼Ÿ

```
"I like turkey"
```

<details>
<summary>ç­”æ¡ˆ</summary>

```
é—®é¢˜ï¼šæ­§ä¹‰

"turkey"æœ‰ä¸¤ä¸ªå«ä¹‰ï¼š
1. åœŸè€³å…¶ï¼ˆå›½å®¶ï¼ŒGPEï¼‰
2. ç«é¸¡ï¼ˆé£Ÿç‰©ï¼Œä¸æ˜¯å®ä½“ï¼‰

æ¨¡å‹å¯èƒ½ï¼š
- é”™è¯¯åœ°è¯†åˆ«ä¸º GPEï¼ˆåœŸè€³å…¶ï¼‰
- æ­£ç¡®åœ°ä¸è¯†åˆ«ä¸ºå®ä½“

ä¸Šä¸‹æ–‡çº¿ç´¢ï¼š
"like" åé¢é€šå¸¸è·Ÿé£Ÿç‰©æˆ–æ´»åŠ¨
â†’ åº”è¯¥æ˜¯ç«é¸¡ï¼Œä¸æ˜¯åœŸè€³å…¶

ä½†æ¨¡å‹å¯èƒ½æ²¡å­¦å¥½è¿™ä¸ªè§„åˆ™
â†’ çœ‹åˆ°å¤§å†™çš„"turkey"å°±è¯†åˆ«ä¸ºå›½å®¶

æ­£ç¡®ç­”æ¡ˆï¼šä¸åº”è¯¥è¯†åˆ«ä¸ºå®ä½“ï¼ˆè¿™é‡ŒæŒ‡é£Ÿç‰©ï¼‰

å¦‚æœæ˜¯"I visited Turkey"
â†’ åº”è¯¥è¯†åˆ«ä¸º GPE
```
</details>

---

## åä¸€ã€Pythonå®ç°è¦ç‚¹

### 1. spaCyåŸºç¡€

```python
import spacy

# åŠ è½½æ¨¡å‹
nlp = spacy.load("en_core_web_sm")

# å¤„ç†æ–‡æœ¬
doc = nlp("Your text here")

# è·å–å®ä½“
entities = [(ent.text, ent.label_) for ent in doc.ents]

# è·å–tokens
tokens = [token.text for token in doc]
```

---

### 2. æ‰¹é‡å¤„ç†ï¼ˆæé«˜æ•ˆç‡ï¼‰

```python
# ä¸é«˜æ•ˆï¼š
for text in texts:
    doc = nlp(text)  # æ¯æ¬¡å•ç‹¬å¤„ç†

# é«˜æ•ˆï¼š
docs = nlp.pipe(texts, batch_size=50)  # æ‰¹é‡å¤„ç†
for doc in docs:
    # å¤„ç†doc
```

---

### 3. Pandasæ•´åˆ

```python
import pandas as pd
import spacy

nlp = spacy.load("en_core_web_sm")

# è¯»å–æ•°æ®
df = pd.read_csv("data.csv")

# åº”ç”¨NER
df['entities'] = df['text'].apply(lambda x: [(ent.text, ent.label_) for ent in nlp(x).ents])

# ç»Ÿè®¡å®ä½“æ•°é‡
df['entity_count'] = df['entities'].apply(len)
```

---

### 4. å¯è§†åŒ–

```python
from spacy import displacy

# å¯è§†åŒ–NERç»“æœï¼ˆJupyter Notebookä¸­ï¼‰
doc = nlp("Barack Obama was born in Hawaii")
displacy.render(doc, style='ent', jupyter=True)

# è¾“å‡ºHTML
html = displacy.render(doc, style='ent', page=True)
with open('ner_result.html', 'w') as f:
    f.write(html)
```

---

## åäºŒã€è®°å¿†å£è¯€

### NERåŸºç¡€
```
äººååœ°åç»„ç»‡åï¼Œ
æ—¥æœŸè´§å¸ä¸èƒ½å¿˜ï¼Œ
å®ä½“è¯†åˆ«æ‰¾å…³é”®ï¼Œ
ä¿¡æ¯æå–ç¬¬ä¸€æ­¥ã€‚
```

### ä¸¤ç§æ–¹æ³•
```
Gazetteeråˆ—è¡¨æŸ¥ï¼Œ
å‡†ç¡®ä½†è¦å¸¸æ›´æ–°ï¼Œ
spaCyç¥ç»ç½‘ç»œå¼ºï¼Œ
æ–°è¯ä¹Ÿèƒ½çŒœä¸€çŒœã€‚
```

### å…±ç°åˆ†æ
```
åŒæ–‡å…±ç°æœ‰è”ç³»ï¼Œ
é¢‘ç‡é«˜åˆ™å…³ç³»å¯†ï¼Œ
ç½‘ç»œå›¾æ¥å¯è§†åŒ–ï¼Œ
ä¸€çœ¼çœ‹æ¸…è°å’Œè°ã€‚
```

---

## åä¸‰ã€å¸¸è§é”™è¯¯

### âŒ é”™è¯¯1ï¼šé¢„å¤„ç†æ—¶è½¬å°å†™

```python
# é”™è¯¯
text = "Barack Obama"
text = text.lower()  # "barack obama"
doc = nlp(text)
# å¯èƒ½æ— æ³•è¯†åˆ«å®ä½“ âŒ

# æ­£ç¡®
text = "Barack Obama"
doc = nlp(text)  # ä¿æŒå¤§å°å†™
# æ­£ç¡®è¯†åˆ« âœ…
```

---

### âŒ é”™è¯¯2ï¼šå¿˜è®°å»é‡

```python
# é”™è¯¯
df = pd.read_csv("data.csv")
# ä¸å»é‡ï¼Œæµªè´¹æ—¶é—´å¤„ç†é‡å¤æ•°æ® âŒ

# æ­£ç¡®
df = pd.read_csv("data.csv")
df = df.drop_duplicates(subset=['text'])  âœ…
```

---

### âŒ é”™è¯¯3ï¼šå…±ç°å¯¹ä¸æ’åº

```python
# é”™è¯¯
cooc[('A', 'B')] += 1
cooc[('B', 'A')] += 1
# ä¸¤ä¸ªä¸åŒçš„é”® âŒ

# æ­£ç¡®
pair = tuple(sorted(['A', 'B']))
cooc[pair] += 1  # æ€»æ˜¯ ('A', 'B')
pair = tuple(sorted(['B', 'A']))
cooc[pair] += 1  # ä¹Ÿæ˜¯ ('A', 'B') âœ…
```

---

### âŒ é”™è¯¯4ï¼šå•ç‹¬å¤„ç†è€Œä¸æ‰¹é‡

```python
# é”™è¯¯ï¼ˆæ…¢ï¼‰
for text in texts:
    doc = nlp(text)  âŒ

# æ­£ç¡®ï¼ˆå¿«ï¼‰
for doc in nlp.pipe(texts):
    # å¤„ç†  âœ…
```

---

**ç¥ä½ è€ƒè¯•é¡ºåˆ©ï¼** ğŸ‰

**Lecture 7æ ¸å¿ƒè¦ç‚¹ï¼š**
- NERï¼šè¯†åˆ«æ–‡æœ¬ä¸­çš„å‘½åå®ä½“
- Gazetteerï¼šåŸºäºåˆ—è¡¨åŒ¹é…ï¼Œå‡†ç¡®ä½†è¦†ç›–æœ‰é™
- spaCyï¼šåŸºäºç¥ç»ç½‘ç»œï¼Œæ³›åŒ–èƒ½åŠ›å¼º
- å®ä½“å…±ç°ï¼šåˆ†æå®ä½“ä¹‹é—´çš„å…³ç³»
- å¤§å°å†™å¾ˆé‡è¦ï¼šå½±å“è¯†åˆ«å‡†ç¡®ç‡
- å¸¸è§å®ä½“ç±»å‹ï¼šPERSON, GPE, ORG, DATE, NORPç­‰

**è®°ä½ï¼š**ç†è§£ä¸¤ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼ŒçŸ¥é“ä½•æ—¶ä½¿ç”¨å“ªç§ï¼ä¼šè®¡ç®—å®ä½“å…±ç°ï¼
